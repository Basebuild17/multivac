# Introduction
DARPA’s Information Innovation Office’s Automating Scientific Knowledge Extraction (ASKE) program seeks to develop approaches to make it easier for scientists to build, maintain and reason over rich models of complex systems — which could include physical, biological, social, engineered or hybrid systems. Gallup’s Meta-model Unification Learned Through Inquiry Vectorization and Automated Comprehension (MULTIVAC) effort supports these goals by developing a system that absorbs scientific knowledge — in the form of facts, relationships, models and equations — from a particular domain corpus into a Markov Logic Network (MLN) ontology and learns to query that ontology in order to accelerate scientific exploration within the target domain. 

In Phase I, Gallup developed a prototype system for ontology induction and query generation. This initial build process has been rich with valuable lessons learned, both about the actual system and system components built so far as well as the upcoming GAN development. As part of a continuous improvement process, documenting lessons learned helps project teams discover the root causes of problems that occurred and avoid those problems in later project stages or future projects. This report formalizes the documentation of several important lessons for the MULTIVAC team from Phase I.

# Lessons Learned
Generally speaking, Gallup believes reducing complexity in approach can help to achieve a minimal viable state, from which improvements can begin. Within MULTIVAC, our team employed concurrent quasi-independent approaches to extract, process and model queries. While that provides a level of robustness and helped accelerate implementation in the Phase I, team members’ code and approaches diverged. For example, multiple natural language processing (NLP) engines were employed at different points to solve distinct but overlapping NLP problems. This allowed the team to solve these problems in parallel, but then presented a much more complex installation and maintenance burden in the finished system. Potentially, by expending somewhat more time up front to standardize on one NLP engine, the team would have been able to speed up later stages of development and optimization.

Relatedly, adopting a more “production” type of mindset would have helped in developing and maintaining a working pipeline for overall execution. Given the multi-faceted needs of the project, much of MULTIVAC was developed in silos. This had advantages in helping people become experts within their domain on the project, but at the expense of overall harmony between components. To finish Phase I took extensive work by a couple of our team members to harmonize code and ensure it ran together as a functioning system.

## Acquiring Data
To build a robust and diverse set of source models, MULTIVAC set a target of 2,000 scientific journal articles fitting set specifications and filters as its “domain.” To achieve this desired source document sample size, MULTIVAC targets three different online repositories: arXiv.org, PubMed, and Springer. In the final version of this prototype system, MULTIVAC scraped a combined 2,740 articles from these three sources.

In the end, housing the system on Amazon Web Services (AWS) has not been necessary to the architecture or performance as expected. The total storage space required is in fact far below the 500GB allocation, and throughput between cloud-based source repositories and local systems is nowhere near the biggest bottleneck for system performance. 

By contrast, the graphical processing unit computational capabilities for the query extraction/generation subsystem associated with our AWS instance set up have been very necessary. AWS will be very useful for scaling up access to this resource over the next phase. 

## Processing and Parsing
Initially, MULTIVAC text parsing relied on two natural language processing engines – stanfordnlp and spaCy – to construct dependency trees, tag parts of speech and lemmatize tokens. MULTIVAC leveraged both NLP engines because of their complementary strengths and value to the overall system. The spaCy library provided unparalleled speed in transforming and manipulating texts, while Stanford’s dependency parsing was intentionally designed to emphasize and prioritize semantically meaningful syntactic structure. 

Despite its performance edge, we found that the spaCy engine dependency tree parsing model and strategy were a poor fit for semantically parsing our texts into a MLN knowledge graph as they are much more purely syntactic in orientation.<sup>[1](#1)</sup> In particular, we found using spaCy resulted in a knowledge graph over-focused on linking and auxiliary verbs such as “is” and “does” with more semantically meaningful verbs like “infect,” “spread,” or “accelerate” relegated to more peripheral sub-trees or terminal node status. This significantly impacted the ability of the MLN system to cluster these terms semantically and create a useful summary of the domain as a knowledge graph. 

However, spaCy did allow for custom vocabulary, which was helpful. Our source texts produced two problems for our dependency parsing – some terms were hyphenated and split between rows, and our equation parsing system introduced “dummy” word tokens in otherwise standard English sentences. It proved extremely difficult to consistently and reliably protect these words from improper processing by Stanford’s NLP engine; when the engine encountered words it did not recognize – especially when those words included non-alphabetical characters – it would try to infer spelling or punctuation corrections to produce in-vocabulary words for processing. With spaCy, we were able to easily add equation “dummy” tokens and other custom terms to the engine’s dictionary and these were then processed as is. 

Relatedly, we encountered some significant differences in performance and results between different generations of Stanford’s dependency parsing toolset. The more mature Java-based corenlp toolset required additional installation and set up to manage the Java server backend, and communication between the Python wrapper client and Java server introduced processing delays. The new PyTorch-based neural net NLP implementation was faster and simpler to install but gave significantly worse results when parsing interrogative sentences. In correspondence with the stanfordnlp authors, it became clear that while the new generation parsing toolset was more advanced in many ways it was also trained on much smaller datasets than the prior corenlp toolset. This is expected to change in the near future, but it continues to be a present-day limitation.

Finally, mapping formal mathematical equations into the same parsed tree structures as the surrounding English natural language text proved to be more straightforward than expected. Both algebraic syntax and Universal Dependencies frameworks are subtypes of dependency grammars, defined as systems which map constituent tokens into a system of directed link dependency trees. This familial resemblance streamlined the process of merging the respective trees into combined structures for ingesting into our MLN knowledge base. 

## Query Generation
MULTIVAC derives queries from source texts using a modified version of deep learning question-generation system called QG-Net.<sup>[2](#2)</sup> QG-Net is a recurrent neural network-based model that takes as inputs a "context" and an "answer" and outputs a question tailored to produce that answer. MULTIVAC uses the most important sentences in the abstract as the "contexts" and uses the most important words and phrases in the sentences as the "answers."

In order to make this system less dependent on thorough annotation of source material and less bound to questions about specific entities, MULTIVAC uses term frequency-inverse document frequency (TF-IDF) scores to determine which sentences – and which terms in those sentences – in the documents are truly important and differentiating. The sentences with the largest total sum of their terms' TF-IDF scores are used to generate queries. Within these sentences, the terms or phrases (by default, the system calculates TF-IDF scores for n-grams up to three words) with the highest TF-IDF scores are tagged as "answers."

Additionally, QG-Net subsystem uses GloVe embeddings to represent words as vectors; MULTIVAC augments the standard pre-trained embeddings with a domain-specific model trained on our corpus, and combines the embeddings using Canonical Correlation Analysis (CCA) on the intersection of tokens between the two models.<sup>[3](#3)</sup> The resulting domain-adapted model encompasses all terms in our corpus and combines semantic meaning from both the domain and wider global context.

From the initial results of MULTIVAC’s Phase I query generation/extraction, most of the queries extracted are relatively straightforward and “factual” in nature. This is not surprising given the motivation of the original QG-Net system, which was to auto-generate quiz questions for textbooks. This does present somewhat of a gap for our purposes between these results and the ideal range of depth, complexity and potential abstraction of scientific research queries we are ultimately interested in producing and modeling. It may be necessary to further develop the QG-Net system to pull in and reproduce more contextual information to achieve more complex queries. Additionally, our team will need to explore alternative approaches – potentially a multistage effort to summarize articles or article sections and then transform these summaries into interrogative forms – to more fundamentally reconceptualize the query generation task.

In terms of the results of MULTIVAC’s Phase I innovations in this space, however, both contributed substantially to producing the level of results we have. Without the domain-adapted GloVe embeddings it is difficult if not impossible for the system to reliably produce queries with highly domain-specific terms – a critical feature for extracting queries that are relevant to much of the most compelling or cutting-edge research. Similarly, the use of TF-IDF scores for guidance on context and answer selection also proved to be essential for the system to work with an un-annotated corpus. Both of these improvements will be important to carry forward in the current phase of development.

Markov Logic Network Construction
From the outputs of the source document processing and parsing components, MULTIVAC builds a probabilistic graphical model knowledge base based on the semantic and syntactic content and structure in the source texts and equations. MULTIVAC extends the prior Markov Logic Network (MLN) designs by integrating the parsed model formulas along with the actual text, mapping both into the same shared ontological space. Thus, the dependencies and relationships in the models, as represented in the mathematical formulas associated with them, are also represented in the MLN knowledge base and enriched by the resulting relationships with the organic contextual knowledge provided by the natural language text. 

These initial ingest and clustering steps are relatively quick, and scale linearly with the input data. However, the next step in the process is to create an agenda for merging or composing semantic clusters across the entire MLN, using maximum a priori (MAP) log likelihood, which in the current system is an operation of O(n2) complexity, unfortunately. Calculating these operations over a sample of 200 of our input articles currently takes about five hours.

One top priority is to leverage Python’s numpy library and vectorization to mitigate the computational cost in this step as much as possible. Additionally, the team is exploring modern graph database systems rather than native Python data structures to store and process the MLN knowledge base data. Graph database systems such as Neo4j would ideally introduce further processing and inference efficiencies, allowing the system to scale up considerably. 

## Generative Adversarial Networks (GANs)
As the central effort in Phase II, MULTIVAC will train a Generative Adversarial Network (GAN) to produce well-formed, novel expert queries without human intervention. While GAN modeling and development has historically been dominated by image generation efforts, in the past couple years there have been increasing numbers of attempts to develop GAN models for text generation. However, these are still highly experimental, and their success has been limited. Gallup's work is looking to study and identify the strengths and weaknesses at this stage of these approaches and in Phase II, looks to build out a GAN for text produced out the QG-Net and MLN portions of MULTIVAC.

Our approach will instead seek to build off the recent work in graph neural networks and tree-based LSTM implementations. Building in knowledge about the dependency tree structures in our training data and desired outputs will help construct syntactically correct queries. Recent work in this area has produced promising results combining source texts with context-free grammar production rule sets to generate realistic SQL or Python programming code.<sup>[4](#4)</sup> Our implementation will require more creativity to capture the far more complex production rules for English language grammar, which is not necessarily context-free. However, a great deal of English text can be produced from a more limited context-free grammar rule set, and it is not clear that uncertainty about the completeness of this solution is a barrier to usefully employing it for our purposes.

# Conclusion
Building on this work from Phase I, Gallup is currently designing and developing the GAN query generation system and incorporating human experts in the design loop. The lessons learned from Phase I will be critical for guiding these new developments in addition to informing improvements and optimizations to the prototype system.

# End Notes
- [<sup><a name='1'>1</a></sup>] c.f. this Issue thread and spaCy NLP library author Matthew Honnibal’s description of the differences: “There's really a need for a more abstract semantic representation on top of the syntactic parse. I'm not sure Stanford's solution of making the parse more semantic is what I like best. I think there's a need for the syntactic representation. It's just that currently, we don't have semantic role labelling. So it's true that the lower-level nature of spaCy's parse makes it difficult to work with in places.” https://github.com/explosion/spaCy/issues/259 
- [<sup><a name='2'>2</sup>] Z. Wang, A. S. Lan, W. Nie, P. Grimaldi, R. Schloss, and R. G. Baraniuk, "QG-Net: A Data-Driven Question Generation Model for Educational Content," ACM Conference on Learning at Scale (L@S), pp. 1-10, June 2018. Full text available at https://people.umass.edu/~andrewlan/papers/18l@s-qgen.pdf
- [<sup><a name='3'>3</sup>] Prathusha K Sarma, YIngyu Liang, William A Sethares, “Domain Adapted Word Embeddings for Improved Sentiment Classification,” Submitted on 11 May 2018. arXiv:1805.04576 [cs.CL] Full text available at: https://arxiv.org/pdf/1805.04576
- [<sup><a name='4'>4</sup>] Xinyue Liu, Xiangnan Kong, Lei Liu, and Kuorong Chiang, “TreeGAN: Syntax-Aware Sequence Generation with
Generative Adversarial Networks,” arXiv preprint arXiv:1808.07582 (2018). https://arxiv.org/abs/1808.07582


