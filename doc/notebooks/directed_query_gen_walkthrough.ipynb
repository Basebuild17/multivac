{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Through for Directed Query Generation\n",
    "This notebook outlines the process of generating novel questions based on a user's seed topic using MULTIVAC's semantic knowledge graph and trained query generator. \n",
    "First, we set up the required imports and arguments for the test. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivac.src.rdf_graph.map_queries import *\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "from multivac.src.gan.gen_test import run\n",
    "os.chdir('src/gan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'dir': os.path.abspath('../../data'),\n",
    "             'out': os.path.abspath('../../models'),\n",
    "             'glove': '../../models/glove.42B.300d',\n",
    "             'run': 'model',\n",
    "             'model': 'transe',\n",
    "             'threshold': 0.1,\n",
    "             'num_top_rel': 10}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load up the knowledge graph embedding model previously calculated. This embedding model allows us to assign probabilities to missing nodes or relationships in the knowledge graph proposed via submitted queries. Here we are using TransE, an approach which models relationships by interpreting them as translations operating on the low-dimensional embeddings of entities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()\n",
    "con.set_in_path(args_dict['dir']+os.path.sep)\n",
    "con.set_work_threads(8)\n",
    "con.set_dimension(100)\n",
    "con.set_test_link_prediction(True)\n",
    "con.set_test_triple_classification(True)\n",
    "\n",
    "files = glob.glob(os.path.join(args_dict['out'],'*tf*'))\n",
    "times = list(set([file.split('.')[2] for file in files]))\n",
    "ifile = max([datetime.strptime(x, '%d%b%Y-%H:%M:%S') for x in times]).strftime('%d%b%Y-%H:%M:%S')\n",
    "con.set_import_files(os.path.join(args_dict['out'], 'model.vec.{}.tf'.format(ifile)))\n",
    "\n",
    "con.init()\n",
    "kem = set_model_choice(args_dict['model'])\n",
    "con.set_model(kem)\n",
    "\n",
    "\n",
    "files = [x for x in os.listdir(con.in_path) if '2id' in x]\n",
    "rel_file = get_newest_file(con.in_path, files, 'relation')\n",
    "ent_file = get_newest_file(con.in_path, files, 'entity')\n",
    "trn_file = get_newest_file(con.in_path, files, 'train')\n",
    "\n",
    "entities = pd.read_csv(ent_file, sep='\\t', \n",
    "                       names=[\"Ent\",\"Id\"], skiprows=1)\n",
    "relations = pd.read_csv(rel_file, sep='\\t', \n",
    "                        names=[\"Rel\",\"Id\"], skiprows=1)\n",
    "train = pd.read_csv(trn_file, sep='\\t', \n",
    "                    names=[\"Head\",\"Tail\",\"Relation\"], skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up a GloVe embedding model. Here we use the large scale, pre-trained GloVe embedding model given the open domain nature of potential submitted questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_vocab, glove_emb = load_word_vectors(args_dict['glove'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we input our seed topic and extract the knowledge graph elements and predicted elements most related to that topic. The system identifies all triples containing the topic or closely semantically related to it, and returns the top `num_top_rel` results (by default, 10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_topic = 'avian flu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = predict_object(con, sample_topic, relations, entities, train, glove_vocab, glove_emb, exact=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are then fed to the query generator, which produces questions in response to each topic. The `run()` function called below does two main things: 1) submit the \"query\" triples to the Generator system to be parsed into a tree object representing the consituency parse of an English language question, and 2) translate that parse into the surface text for presentation:\n",
    "```python\n",
    "    results = netG.parse(query, beam_size=netG.args['beam_size'])\n",
    "    texts = [asdl_ast_to_english(x.tree) for x in results]\n",
    "\n",
    "    return texts\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = results.Text.apply(lambda x: run({'query': list(x), \n",
    "                                              'model': os.path.join(args_dict['out'], 'gen_checkpoint.pth')}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
