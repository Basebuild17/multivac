{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIVAC - Meta-model Unification Learned Through Inquiry Vectorization and Automated Comprehension\n",
    "## Introduction\n",
    "Gallup’s MULTIVAC effort supports the goals of the DARPA ASKE program by developing a system that absorbs scientific knowledge — in the form of facts, relationships, models and equations — from a particular domain corpus into a Markov Logic Network (MLN) ontology and learns to query that ontology in order to accelerate scientific exploration within the target domain. MULTIVAC will consist of an expert query generator trained on a corpus of historical expert queries and tuned dialectically with the use of a Generative Adversarial Network (GAN) architecture. As a prototype system, MULTIVAC will focus on the domain of epidemiological research, and specifically the realm of SIR/SEIR (Susceptible-Infected-Recovered, often with an additional “Exposed” element) compartmental model approaches. It is Gallup’s intent that this system includes a “human-in-the-loop” element, especially during training, to ensure that the system is properly tuned and responsive to the needs and interests of the human researchers it is intended to augment.\n",
    "\n",
    "DARPA’s Information Innovation Office’s Automating Scientific Knowledge Extraction (ASKE) program seeks to develop approaches to make it easier for scientists to build, maintain and reason over rich models of complex systems — which could include physical, biological, social, engineered or hybrid systems. By interpreting and exposing scientific knowledge and assumptions in existing model code and documentation, researchers can identify new data and information resources automatically, extracting useful information from these sources, and integrating this useful information into machine-curated expert models for robust modeling.\n",
    "\n",
    "## Replication\n",
    "The MULTIVAC pipeline, to run from beginning to end, is encapsulated in command-line executable code found on Gallup's [MULTIVAC repository](https://github.com/GallupGovt/multivac). To run that file -- `conductor.py`, found in the top-level MULTIVAC directory -- one needs to do the following:\n",
    "1. Within the [MULTIVAC repository](https://github.com/GallupGovt/multivac):\n",
    "  * Clone the Gallup MULTIVAC repository\n",
    "  * Instantiate a virtual environment\n",
    "  * `pip install -r requirements.txt` to install all necessary Python dependencies\n",
    "2. Within the [QG-Net repository](https://github.com/GallupGovt/qgnet) (a secondary, cloned repository that the Gallup team uses for a portion of its modeling effort):\n",
    "  * Clone the Gallup QG-Net repository (this should be at the same level directory as MULTIVAC)\n",
    "  * Clone the Facebook Research [DrQA](https://github.com/facebookresearch/DrQA) repository and follow the [install instructions](https://github.com/facebookresearch/DrQA#installing-drqa) (this should be at the same level directory as MULTIVAC). Note, this requires a Linux/OSX machine.\n",
    "3. Other downloads and dependencies\n",
    "  * The [Stanford Core NLP](https://stanfordnlp.github.io/CoreNLP/download.html) toolset needs to be downloaded and installed. **Be sure to follow the setup instructions** to ensure proper install for this to work.\n",
    "  * The [GloVe pre-trained data](http://nlp.stanford.edu/data/glove.42B.300d.zip) need to be downloaded and placed in the `data_dir` as defined in `settings.py` in the top-level directory of the MULTIVAC repository. \n",
    "  * Have [R](https://cran.r-project.org/) installed on your machine; part of MULTIVAC's code reaches back into R.\n",
    "\n",
    "Finally, QG-Net specifically, but the project overall, works on a GPU. **The QG-Net portion of code will not work unless on a GPU** (tested with an Nvidia Quadro Pro-4000 card) but other pieces, especially the Markov logic network (MLN) will have dramatically reduced performance on a CPU. Also note, end-to-end, this will take approximately 60 hours to complete. \n",
    "\n",
    "## Piece-by-piece execution\n",
    "If one chooses not to replicate the entire pipeline, then below are pieces to step in and out of particular pieces, given data pre-compiled by Gallup. These files can be accessed through a public, non-secure FTP site hosted at Gallup. Navigate to ftp://ftp.gallup.com in a web browser. You will be promopted for credentials that Ben Ryan will have sent over. From there, navigate to the `aske` folder where you will see all of the various inputs and outputs of the MULTIVAC system. The instructions are as follows with what to do with these data:\n",
    "* Scraping\n",
    "  * Inputs: There are no inputs per se, since this first step instantiates MULTIVAC by scraping articles from the internet from arXiv, Springer, and PubMed around the epidemilogy topic of choice for MULTIVAC.\n",
    "  * Outputs: The `20181212.json` file encompasses all of the scraped data and are used by the parsing step.\n",
    "* Parsing\n",
    "  * Inputs: The `20181212.json` file.\n",
    "  * Outputs: The `articles-with-equations.json` and the files in the `dim_files` directory. The JSON file is used with GloVe modeling and QG-Net, which the DIM files are used with the MLN.\n",
    "* GloVe modeling\n",
    "  * Inputs: The `articles-with-equations.json` file.\n",
    "  * Outputs: The `da_embeddings.txt` file. This is a domain-adapted word embeddings used in QG-Net.\n",
    "* QG-Net\n",
    "  * Inputs: The `da_embeddings.txt` and the `articles-with-equations.json` files.\n",
    "  * Outputs: The `input.txt` is a processed version of data that is fed through QG-Net itself and the `output_questions_QG-Net.pt.txt` is a set of questions from QG-Net.\n",
    "* MLN\n",
    "  * Inputs: The files in the `dim_files` directory.\n",
    "  * Outputs: The `mln.pkl` file that wraps up the knowledge graph and attributes, including semantic clustering.\n",
    "\n",
    "The rest of this notebook will step through language around the particular component of MULTIVAC and then code -- using the data referenced above -- to exemplify execution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Settings\n",
    "In the cell below, update the directory list as needed, following the example text. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import configparser\n",
    "import os\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "\n",
    "from multivac.src import utilities\n",
    "\n",
    "\n",
    "cfg = configparser.ConfigParser()\n",
    "cfgDIR = Path('').resolve()\n",
    "\n",
    "try:\n",
    "    cfg.read(cfgDIR / config_file_name)\n",
    "except NameError:\n",
    "    cfg.read(cfgDIR / 'multivac.cfg')\n",
    "\n",
    "root_dir = cfg['PATHS'].get('root_dir', cfgDIR/'multivac')\n",
    "qgnet_dir = cfg['PATHS'].get('qgnet_dir', cfgDIR/'qgnet')\n",
    "\n",
    "data_dir = cfg['PATHS'].get('data_dir', root_dir/'data')\n",
    "raw_dir = cfg['PATHS'].get('raw_dir', data_dir/'raw')\n",
    "interim_dir = cfg['PATHS'].get('interim_dir', data_dir/'interim')\n",
    "processed_dir = cfg['PATHS'].get('processed_dir', data_dir/'processed')\n",
    "metadata_dir = cfg['PATHS'].get('metadata_dir', processed_dir/'metadata')\n",
    "models_dir = cfg['PATHS'].get('models_dir', root_dir/'models')\n",
    "stanf_nlp_dir = cfg['PATHS'].get('stanf_nlp_dir',\n",
    "                                 root_dir/'stanford_nlp_models')\n",
    "mln_dir = cfg['PATHS'].get('mln_dir', root_dir/'mln_models')\n",
    "\n",
    "# Get search and filter settings; default to empty lists\n",
    "terms = eval(cfg['SEARCH'].get('terms', '[]'))\n",
    "sources = eval(cfg['SEARCH'].get('sources', '[]'))\n",
    "arxiv_drops = eval(cfg['FILTER'].get('drops', '[]'))\n",
    "\n",
    "# make data directories if they don't already exist\n",
    "dirs = [\n",
    "    data_dir,\n",
    "    raw_dir,\n",
    "    interim_dir,\n",
    "    processed_dir,\n",
    "    metadata_dir,\n",
    "    models_dir,\n",
    "    stanf_nlp_dir,\n",
    "    mln_dir,\n",
    "]\n",
    "dirs += [raw_dir / x for x in sources]\n",
    "for _dir in dirs:\n",
    "    utilities.mkdir(_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivac.src import utilities\n",
    "\n",
    "\n",
    "# make data directories if they don't already exist\n",
    "dirs = [\n",
    "    data_dir,\n",
    "    raw_dir,\n",
    "    interim_dir,\n",
    "    processed_dir,\n",
    "    metadata_dir,\n",
    "    models_dir,\n",
    "    stanf_nlp_dir,\n",
    "    mln_dir,\n",
    "]\n",
    "dirs += ['{}/{}'.format(raw_dir, x) for x in sources]\n",
    "for _dir in dirs:\n",
    "    utilities.mkdir(_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Scraping\n",
    "To build a robust and diverse set of source models, MULTIVAC set a target of 2,000 scientific journal articles fitting set specifications and filters. These specifications are coded in a user-editable configuration file (by default, `multivac.cfg`), and cover both search parameters (search terms, and sources to search) as well as filter parameters, to weed out duplicates or unrelated content that might return in a more naive match on the specified search terms.\n",
    " \n",
    "To achieve the desired sample document size, MULTIVAC targets three different online sources of epidemiological research: arXiv.org, PubMed, and Springer. MULTIVAC accesses these resources through source-specific APIs and authenticates with user API access keys. Each source is searched for articles containing the specified search terms The results are saved as a combined JSON file. Each article ID is a top-level key, with a Metadata key containing various metadata keys and values and a Text key containing the plain body text. This file then serves as the intermediate \"source\" datastore for subsequent analysis and processing.\n",
    " \n",
    "These three sources also serve to demonstrate MULTIVAC's ability to work with a variety of data storage types: PubMed articles are ingested from XML, Springer from HTML, and arXiv from PDF. In the final version of this prototype system, MULTIVAC scraped a combined 2,740 articles (after pruning malformed or un-parseable results) from these three sources: 897 from arXiv, 1,280 from PubMed, and 573 from Springer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.A. Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import feedparser\n",
    "import os\n",
    "import pickle\n",
    "import requests\n",
    "import time\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "\n",
    "from multivac import settings\n",
    "from multivac.src.data.get import get_total_number_of_results, prep_terms, query_api\n",
    "\n",
    "\n",
    "# load environment variables from .env\n",
    "springer_api_key = os.environ.get('SPRINGER_API_KEY')\n",
    "user_email = os.environ.get('USER_EMAIL')  # courtesy to NIH to include your email\n",
    "\n",
    "wait_time = 3\n",
    "\n",
    "\n",
    "def collect_get_main():\n",
    "    # ------------------------------------------------------------------------\n",
    "    # arxiv\n",
    "\n",
    "    # build query and get metadata of articles matching our search criteria\n",
    "    params = {'start': 0, 'max_results': 100, 'sortBy': 'relevance'\n",
    "             ,'sortOrder': 'descending'}\n",
    "    li = [x.replace('-', ' ').split(' ') for x in settings.terms]\n",
    "    q = 'OR'.join(['%28' + prep_terms(x) + '%29' for x in li])\n",
    "    url = 'http://export.arxiv.org/api/query?search_query=' + q\n",
    "    arxiv_metadata = query_api(url, q, params, wait_time=1, verbose=True)\n",
    "\n",
    "    # save pdfs of articles that matched our search criteria\n",
    "    # we use doi as the filename when that id is present; otherwise we use the\n",
    "    # arxiv id\n",
    "    for ix, md in enumerate(arxiv_metadata):\n",
    "        url = md['id']\n",
    "        pdf_url = url.replace('/abs/', '/pdf/')\n",
    "        article_fn = url.split('/abs/')[-1]\n",
    "        article_fn = '_'.join(article_fn.split('/')) + '.pdf'\n",
    "        # specify filename so we can associate each pdf with its metadata down\n",
    "        # the road\n",
    "        arxiv_metadata[ix]['fn'] = article_fn\n",
    "        dst = settings.raw_dir / 'arxiv' / article_fn\n",
    "        if not os.path.exists(dst):\n",
    "            r = requests.get(pdf_url)\n",
    "            with open(dst, 'wb') as f:\n",
    "                f.write(r.content)\n",
    "            time.sleep(0.3)\n",
    "\n",
    "    # save arxiv metadata\n",
    "    fn = 'arxiv' + '.pkl'\n",
    "    dst = settings.metadata_dir / fn\n",
    "    with open(dst, 'wb') as f:\n",
    "        pickle.dump(arxiv_metadata, f)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # springer\n",
    "\n",
    "    # build query to retrieve metadata\n",
    "    make_q = lambda li: '(' + ' OR '.join(['\"' + s + '\"' for s in li]) + ')'\n",
    "    q = make_q(settings.terms)\n",
    "    base = 'http://api.springernature.com/openaccess/json?q='\n",
    "    url = base + q\n",
    "    params = {\n",
    "        'source': 'springer',\n",
    "        'openaccess': 'true',\n",
    "        'api_key': springer_api_key, 'p': 20, 's': 1\n",
    "    }\n",
    "    params_ = copy.deepcopy(params)\n",
    "\n",
    "    # retrieve metadata\n",
    "    springer_metadata = []\n",
    "    while True:\n",
    "        r = requests.get(url, params_)\n",
    "        if len(r.json()['records']) == 0:\n",
    "            break\n",
    "        params_['s'] = params_['s'] + params_['p']\n",
    "        springer_metadata += r.json()['records']\n",
    "        time.sleep(wait_time)\n",
    "    print('%s total Springer articles' % len(springer_metadata))\n",
    "\n",
    "    # iterate over springer metadata and download html for each article\n",
    "    # we use a generator to increase wait times with each connection error\n",
    "    waits = (2**x for x in range(0,6))\n",
    "    for ix, md in enumerate(springer_metadata):\n",
    "        fn = md['doi'].replace('/', '-')\n",
    "        if len(fn) == 0:\n",
    "            fn = md['identifier']\n",
    "        fn = fn + '.html'\n",
    "        springer_metadata[ix]['fn'] = fn\n",
    "        dst = settings.raw_dir / 'springer' / fn\n",
    "        if not os.path.exists(dst):\n",
    "            try:\n",
    "                r = requests.get(md['url'][0]['value'])\n",
    "            except ConnectionError:\n",
    "                time.sleep(waits.__next__)\n",
    "                r = requests.get(md['url'][0]['value'])\n",
    "            html = bs(r.text).encode('utf-8').decode('utf-8')\n",
    "            with open(dst, 'w', encoding='utf-8') as f:\n",
    "                f.write(html)\n",
    "            time.sleep(3)\n",
    "\n",
    "    # save springer metadata\n",
    "    dst = settings.metadata_dir / 'springer.pkl'\n",
    "    with open(dst, 'wb') as f:\n",
    "        pickle.dump(springer_metadata, f)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # pubmed\n",
    "\n",
    "    # search pubmed central for free full text articles containing selected\n",
    "    # query\n",
    "    # get the ids which we then use to get the xml text data\n",
    "    replace = lambda s: s.replace(' ', '+')\n",
    "    quote = lambda s: '%22' + s + '%22'\n",
    "    terms = [quote(replace(s)) for s in settings.terms]\n",
    "    term = 'term='+ '%28'+ '+OR+'.join(terms) + '%29'\n",
    "    fulltext = 'free+fulltext%5bfilter%5d'\n",
    "    retmax = 'retmax=2000'\n",
    "    base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc'\n",
    "    params = {'retmax': 5000, 'email': user_email}\n",
    "    url = base + '&' + term + '+' + fulltext + '&' + retmax\n",
    "    r = requests.get(url)\n",
    "    ids = [x.contents[0] for x in bs(r.text).find_all('id')]\n",
    "\n",
    "    print('%s Pubmed Central (PMC) articles' % ids)\n",
    "\n",
    "    # get xml text data and save to disk\n",
    "    for i in ids:\n",
    "        pmc_id = 'pmc' + str(i)\n",
    "        fn = (pmc_id + '.xml')\n",
    "        dst = settings.raw_dir / 'pubmed' / fn\n",
    "        if not os.path.exists(dst):\n",
    "            url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=' + str(i)\n",
    "            r = requests.get(url, params={'id': i})\n",
    "            xml = r.text\n",
    "            with open(dst, 'w') as f:\n",
    "                f.write(xml)\n",
    "            time.sleep(0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "988 total results, 1 second wait time between each call\n",
      "*"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b1b112b3acbb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcollect_get_main\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-be836e31f595>\u001b[0m in \u001b[0;36mcollect_get_main\u001b[0;34m()\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'OR'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'%28'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mprep_terms\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'%29'\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mli\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m     \u001b[0murl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'http://export.arxiv.org/api/query?search_query='\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 31\u001b[0;31m     \u001b[0marxiv_metadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mquery_api\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwait_time\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0;31m# save pdfs of articles that matched our search criteria\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/git/multivac/src/data/get.py\u001b[0m in \u001b[0;36mquery_api\u001b[0;34m(url, terms, params, wait_time, verbose)\u001b[0m\n\u001b[1;32m    190\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    191\u001b[0m             \u001b[0mmetadata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mentries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 192\u001b[0;31m         \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mwait_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    193\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m''\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    194\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mmetadata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "collect_get_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.B. Parse results (note, Part 1.A. must be run first)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import pubmed_parser\n",
    "import slate\n",
    "\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "\n",
    "from multivac import settings\n",
    "from multivac.src import utilities\n",
    "from multivac.src.data.process import (\n",
    "    aggregate_pubmed, filter_arxiv, parse_articles_data, parse_html, parse_pdf, parse_pubmed, save_outputs\n",
    ")\n",
    "\n",
    "\n",
    "def collect_process_main():\n",
    "    output = {}\n",
    "    for source in settings.sources:\n",
    "        data_raw_dir = settings.raw_dir / source\n",
    "        if source in ['arxiv', 'springer']:\n",
    "            data = parse_articles_data(source, data_raw_dir)\n",
    "        elif source == 'pubmed':\n",
    "            srcs = [data_raw_dir / x for x in os.listdir(data_raw_dir)]\n",
    "            data = aggregate_pubmed(srcs)\n",
    "        if len(output) == 0:\n",
    "            output = copy.deepcopy(data)\n",
    "        else:\n",
    "            output.update(data)\n",
    "    arxiv_drops = [x.split()[0] for x in settings.arxiv_drops]\n",
    "    filtered_output = filter_arxiv(output, arxiv_drops)\n",
    "    save_outputs(filtered_output)\n",
    "    return True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "collect_process_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Parsing\n",
    "The parsing component of MULTIVAC takes a JSON file of scraped journal articles, and parses the text as well as LaTeX notation contained within the text. It is largely driven by parsing.py which accepts five arguments:\n",
    "-b\n",
    "OPTIONAL\n",
    "Specify the index of the first document to start processing (useful for stopping and continuing the parsing process)\n",
    " \n",
    "-s\n",
    "REQUIRED\n",
    "Path to the Stanford NLP model\n",
    "-c\n",
    "REQUIRED\n",
    "Indicator for whether or not to create a JSON file with tokenized LaTeX equations (y/n)\n",
    "-d\n",
    "REQUIRED\n",
    "Path to JSON input file\n",
    "-o\n",
    "REQUIRED\n",
    "Path where dependency, input and morphology files should be written. The folder must contain three subfolders labeled dep, input and morph.\n",
    " \n",
    "Initial MULTIVAC text parsing relies on two natural language processing engines – stanfordnlp and spaCy – to construct dependency trees, tag parts of speech and lemmatize tokens. MULTIVAC leverages both NLP engines because of their complementary strengths and value to the overall system. The spaCy library provides unparalleled speed in transforming and manipulating texts, while Stanford’s dependency parsing is intentionally designed to emphasize and prioritize semantically meaningful syntactic structure. While spaCy also performs dependency tree parsing, the model and strategy are much more purely syntactic, making it a poor fit for semantically parsing our texts into a Markov Logic network knowledge graph.\n",
    " \n",
    "Each sentence is processed individually to identify the dependency structure of its tokens. When LaTeX notation occurs in text the notation block is extracted and a “dummy” token is substituted, allowing the NLP dependency parsing to interpret the sentence as a proper English language construct. This is especially important for in-line LaTeX notations, which otherwise render many of the most important sentences in an article un-parseable.\n",
    "\n",
    "![LaTeX equation parsing](images/latex_parse_1.png)\n",
    "\n",
    "The LaTeX equation itself is separately parsed and then re-inserted into the sentence, with the root of the LaTeX tree taking the place of the dummy token in the dependency structure. The LaTeX representations are parsed by converting them first into a sympy representation that enables deconstructing expressions into a nested tree structure that contains a series of functions and arguments. For example, the expression 2x + (x*y) would be expressed as Add(Pow(Number(2), Entity('x')), Mul(Entity('x'), Entity('y'))) where Add(), Pow() and Mul() are functions; and Number(2) and Symbol(‘x’) are arguments. MULTIVAC transforms these nested parenthetical representations into a collapsed dependencies format and inserts the entire chain back into the source sentence, updating token indices as appropriate. The individual relationship and entity tokens from these equations are also expanded out in string representation and replace the LaTeX notation in the original text.\n",
    "\n",
    "![LaTeX entity relationshop](images/latex_parse_2.png)\n",
    "\n",
    "The outputs of this translation process are three sets of files: Dependency files, Morphology files, and Input files. Each file represents a parse of one article and is formatted in blocks, with one block for each sentence in the article. “Input” files record original word or punctuation as well as the part of speech (POS) tag, while “Morph” files record the token lemma, and each line contains a separate token. “Dep” files record the Stanford Universal Dependency relationships between pairs of words as well as the indices of the component words in the sentence. The article texts with processed equation tokens re-inserted are also written out to a file (articles-with-equations.json) for further use in preparing GloVe embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (equationparsing.py, line 317)",
     "output_type": "error",
     "traceback": [
      "Traceback \u001b[0;36m(most recent call last)\u001b[0m:\n",
      "  File \u001b[1;32m\"/usr/local/lib/python3.6/site-packages/IPython/core/interactiveshell.py\"\u001b[0m, line \u001b[1;32m3265\u001b[0m, in \u001b[1;35mrun_code\u001b[0m\n    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-8-3adefbb6ad0b>\"\u001b[0;36m, line \u001b[0;32m8\u001b[0;36m, in \u001b[0;35m<module>\u001b[0;36m\u001b[0m\n\u001b[0;31m    import multivac.src.data.equationparsing as eq\u001b[0m\n",
      "\u001b[0;36m  File \u001b[0;32m\"/Users/matt_hoover/git/multivac/src/data/equationparsing.py\"\u001b[0;36m, line \u001b[0;32m317\u001b[0m\n\u001b[0;31m    l_morTokens_latex_sub = latexParsing(token, 0)\u001b[0m\n\u001b[0m                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "import json\n",
    "import pickle\n",
    "import re as reg\n",
    "import spacy\n",
    "import stanfordnlp\n",
    "\n",
    "import multivac.src.data.equationparsing as eq\n",
    "\n",
    "from interruptingcow import timeout\n",
    "\n",
    "from multivac import settings\n",
    "from multivac.src.data.textparsing import clean_doc\n",
    "from multivac.src.data.parsing import (\n",
    "    create_parse_files, get_adjustment_position, get_token_governor, \n",
    "    load_data\n",
    ")\n",
    "\n",
    "def nlp_parse_main(args_dict):\n",
    "    ''' Main run file that orchestrates everything\n",
    "    '''\n",
    "\n",
    "    ## Load NLP engines\n",
    "    spacynlp = spacy.load('en_core_web_sm')\n",
    "    nlp = stanfordnlp.Pipeline(models_dir=settings.stanf_nlp_dir,\n",
    "                               treebank='en_ewt', use_gpu=False,\n",
    "                               pos_batch_size=3000)\n",
    "\n",
    "    ## Load documents\n",
    "    jsonObj, allDocs = load_data(settings.processed_dir / 'data')\n",
    "\n",
    "    ## Process and Clean documents\n",
    "    try:\n",
    "        allDocsClean = pickle.load(open('allDocsClean.pkl', \"rb\" ))\n",
    "        print('Loaded pickle!')\n",
    "    except FileNotFoundError:\n",
    "        print('Starting from scratch')\n",
    "        allDocsClean= []\n",
    "        for i, doc in enumerate(allDocs):\n",
    "            if i%10==0:\n",
    "                print(i)\n",
    "            allDocsClean.append(clean_doc(doc, spacynlp))\n",
    "\n",
    "        with open('allDocsClean.pkl', 'wb') as f:\n",
    "            pickle.dump(allDocsClean, f)\n",
    "\n",
    "\n",
    "    allDocs2 = [eq.extract_and_replace_latex(doc) for docNum, doc in\n",
    "                enumerate(allDocsClean)]\n",
    "    print('Number of LateX Equations parsed: {}'.format(len(eq.LATEXMAP)))\n",
    "\n",
    "\n",
    "    ## Put equations back into text - this will be fed to glove embedding\n",
    "    if args_dict['nlp_newjson']:\n",
    "        print('***************\\nBuilding JSON file for glove embedding...')\n",
    "        allDocs3 = []\n",
    "        percentCompletedMultiple = int(len(allDocs2)/10)\n",
    "        for i, doc in enumerate(allDocs2[0:]):\n",
    "            if i%percentCompletedMultiple == 0:\n",
    "                print('{}% completed'.format(round(i/(len(allDocs2))*100, 0)))\n",
    "            newDoc = reg.sub(r'Ltxqtn[a-z]{8}', eq.put_equation_tokens_in_text,\n",
    "                             doc)\n",
    "            allDocs3.append(newDoc)\n",
    "\n",
    "        jsonObj2 = copy.deepcopy(jsonObj)\n",
    "        allDocs3Counter = 0\n",
    "\n",
    "        for key, value in list(jsonObj2.items()):\n",
    "            if value['text']:\n",
    "                jsonObj2[key]['text']=allDocs3[allDocs3Counter]\n",
    "                allDocs3Counter = allDocs3Counter+1\n",
    "\n",
    "        with open('{}/articles-with-equations.json'.format(settings.data_dir),\n",
    "                  'w', encoding='utf8') as fp:\n",
    "            json.dump(jsonObj2, fp)\n",
    "\n",
    "\n",
    "    ## Parse files into DIM\n",
    "    startPoint=-1\n",
    "    if args_dict['nlp_bp'] is not None:\n",
    "        startPoint = args_dict['nlp_bp']\n",
    "\n",
    "    for i, doc in enumerate(allDocs2[0:]):\n",
    "        print('Processing document #{}'.format(i))\n",
    "        if i > startPoint:\n",
    "\n",
    "            # Use exception handling so that the process doesn't get stuck and\n",
    "            # time out because of memory errors\n",
    "            try:\n",
    "                with timeout(300, exception=RuntimeError):\n",
    "                    nlpifiedDoc = nlp(doc)\n",
    "                    thisDocumentData = create_parse_files(\n",
    "                        nlpifiedDoc, i, True, settings.data_dir\n",
    "                    )\n",
    "            except RuntimeError:\n",
    "                print(\"Didn't finish document #{} within five minutes. Moving to next one.\".format(i))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. GloVe models\n",
    "MULTIVAC also trains a 300-dimensional domain-adapted Global Vectors (GloVe) word-embeddings model on the corpus and saves this file in the same folder. GloVe embeddings derive multi-dimensional vector spaces describing word associations based on calculations of word co-occurrences over a large corpus.<sup>[1]</sup> \n",
    " \n",
    "MULTIVAC begins with a pre-trained 300-dimensional GloVe model incorporating 2 million terms found in the Common Crawl corpus, a collection of over 2 billion webpages scraped monthly.<sup>[2]</sup> This model represents a best-in-class embedding model for generic English language text. However, given the specific and highly technical domain we are attempting to understand and model, much domain-specific semantic knowledge – not to mention domain-specific vocabulary – are not accounted for in this generic model. MULTIVAC augments this model by training a domain-specific model on our corpus, and combining embeddings using Canonical Correctional Analysis (CCA) on the intersection of tokens between the two models.<sup>[3]</sup> The vectors for each token of the domain adapted GloVe embedding model are derived from a weighted average of the canonical vectors (N = 100) from the CCA analysis.\n",
    " \n",
    "This alignment occurs on words that exist in both the domain-specific and generic model vocabularies, but for terms that are entirely domain-specific the vector representations are projected into the 100-dimensional canonical vector space from the CCA analysis via matrix multiplication and appended to the domain-adapted embedding vectors. The resulting domain-adapted model encompasses all terms in our corpus and combines semantic meaning from both the domain and wider global context.\n",
    "\n",
    "<hr>\n",
    "\n",
    "<sup>[1]</sup> Jeffrey Pennington, Richard Socher, and Christopher D. Manning. 2014. “GloVe: Global Vectors for Word Representation.” Full text available at https://nlp.stanford.edu/pubs/glove.pdf\n",
    "\n",
    "<sup>[2]</sup> See http://commoncrawl.org/connect/blog/ for up to date statistics on the corpus. As of this report the total is now 3.1 billion pages, though this has varied over time since project inception, and not simply increased monotonically. When the pre-trained GloVe model was created the corpus was closer to 2 billion pages in size.\n",
    "\n",
    "<sup>[3]</sup> Prathusha K Sarma, YIngyu Liang, William A Sethares, “Domain Adapted Word Embeddings for Improved Sentiment Classification,” Submitted on 11 May 2018. arXiv:1805.04576 [cs.CL] Full text available at: https://arxiv.org/pdf/1805.04576"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
