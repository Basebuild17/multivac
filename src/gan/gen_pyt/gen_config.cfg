
#### General configuration ####
seed = 0
cuda = False
lang = eng
asdl_file = None
mode = train

#### Modularized configuration ####
parser = default_parser
transition_system = english
evaluator = default_evaluator

#### Model configuration ####
lstm = lstm

# Embedding sizes
embed_size = 128
action_embed_size = 128
field_embed_size = 64
type_embed_size = 64

# Hidden sizes
hidden_size = 256
ptrnet_hidden_dim = 32
att_vec_size = 256

# readout layer
no_query_vec_to_action_map = False
readout = identity
query_vec_to_action_diff_map = False

# supervised attention
sup_attention = False

# parent information switch for decoder LSTM
no_parent_production_embed = False
no_parent_field_embed = False
no_parent_field_type_embed = False
no_parent_state = False

no_input_feed = False
no_copy = False

# Model configuration parameters specific for wikisql
column_att = affine
answer_prune = True
no_answer_prune = False

#### Training ####
vocab = None
glove_embed_path = None

train_file = None
dev_file = None

batch_size = 10
dropout = 0.
word_dropout = 0.
decoder_word_dropout = 0.3
primitive_token_label_smoothing = 0.0
src_token_label_smoothing = 0.0

negative_sample_type = best

# training schedule details
valid_metric = acc
valid_every_epoch = 1
log_every = 10

save_to = model
save_all_models = False
patience = 5
max_num_trial = 10
uniform_init = None
glorot_init = False
clip_grad = 5.
max_epoch = -1
optimizer = Adam
lr = 0.001
lr_decay = 0.5
lr_decay_after_epoch = 0
decay_lr_every_epoch = False
reset_optimizer = False
verbose = False
eval_top_pred_only = False

#### decoding/validation/testing ####
load_model = None
beam_size = 5
decode_max_time_step = 100
sample_size = 5
test_file = None
save_decode_to = None

#### dataset specific config ####
sql_db_file = None

