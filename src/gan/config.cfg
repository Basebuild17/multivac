[GAN]
ROLLOUT_UPDATE_RATE = 0.8
ROLLOUT_NUM = 5

G_STEPS = 2
D_STEPS = 2
K_STEPS = 2

SEED = 88
BATCH_SIZE = 64
TOTAL_EPOCHS = 2
GENERATED_NUM = 100
VOCAB_SIZE = 10
SEQUENCE_LEN = 20

data_dir = data
glove_dir = ../../models
glove_file = DA_glove_embeddings_300.pkl
data_type = eng
verbose = True

[GENERATOR]
seed = 0
cuda = False
lang = eng
mode = train
parser = default_parser
transition_system = english
evaluator = default_evaluator

annot_file = data/query_annots.txt
texts_file = data/query_texts.txt
grammar = None
sample_dir = data/samples
output_dir = ../../models
verbose = True
PRE_G_EPOCHS = 5
G_EMB_SIZE = 32
G_HIDDEN_SIZE = 32
G_LR = 1e-3
G_OPTIM = Adam

#### Model configuration ####
lstm = lstm

# Embedding sizes
embed_size = 128
action_embed_size = 128
field_embed_size = 64
type_embed_size = 64

# Hidden sizes
hidden_size = 256
ptrnet_hidden_dim = 32
att_vec_size = 256

# readout layer
no_query_vec_to_action_map = False
readout = identity
query_vec_to_action_diff_map = False

# supervised attention
sup_attention = False

# parent information switch for decoder LSTM
no_parent_production_embed = False
no_parent_field_embed = False
no_parent_field_type_embed = False
no_parent_state = False

no_input_feed = False
no_copy = False
vocab = None
glove_embed_path = None

train_file = None
dev_file = None

batch_size = 10
dropout = 0.
word_dropout = 0.
decoder_word_dropout = 0.3
primitive_token_label_smoothing = 0.0
src_token_label_smoothing = 0.0

negative_sample_type = best

# training schedule details
valid_metric = acc
valid_every_epoch = 1
log_every = 10

save_to = model
save_all_models = False
patience = 5
max_num_trial = 10
uniform_init = None
glorot_init = False
clip_grad = 5.
max_epoch = -1
optimizer = Adam
lr = 0.001
lr_decay = 0.5
lr_decay_after_epoch = 0
decay_lr_every_epoch = False
reset_optimizer = False
eval_top_pred_only = False

#### decoding/validation/testing ####
load_model = None
beam_size = 5
decode_max_time_step = 100
sample_size = 5
test_file = None
save_decode_to = None


[DISCRIMINATOR]
data = discriminator/data/multivac
save = discriminator/checkpoints
expname = test

# model arguments
vocab_size = 0
input_dim = 300
mem_dim = 150
hidden_dim = 50
freeze_embed = True

# training arguments
epochs = 15
batchsize = 25
lr = 0.01
wd = 1e-4
sparse = False
optim = adagrad
