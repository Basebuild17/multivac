[GAN]
ROLLOUT_UPDATE_RATE = 0.8
ROLLOUT_NUM = 2

G_STEPS = 1
D_STEPS = 1
K_STEPS = 4

SEED = 12
BATCH_SIZE = 64
TOTAL_EPOCHS = 200
GENERATED_NUM = 100
VOCAB_SIZE = 10
SEQUENCE_LEN = 10

data_dir = data
glove_dir = ../../models
glove_file = DA_glove_embeddings_300.pkl
glove_lower = True
data_type = eng
verbose = True

[GENERATOR]
seed = 12
cuda = False
lang = eng
mode = train
parser = default_parser
transition_system = english
evaluator = default_evaluator

annot_file = data/query_annots.txt
texts_file = data/query_texts.txt
grammar = None
sample_dir = data/samples
output_dir = ../../models
verbose = True
PRE_G_EPOCHS = 50
G_EMB_SIZE = 16
G_HIDDEN_SIZE = 16
G_LR = 0.0001
G_OPTIM = Adam

#### Model configuration ####
lstm = lstm
encoder = cnn

# Embedding sizes
embed_size = 128
action_embed_size = 128
field_embed_size = 64
type_embed_size = 64

# Hidden sizes
hidden_size = 256
ptrnet_hidden_dim = 32
att_vec_size = 256

# readout layer
no_query_vec_to_action_map = False
readout = sqnl
query_vec_to_action_diff_map = False

# supervised attention
sup_attention = False

# parent information switch for decoder LSTM
no_parent_production_embed = False
no_parent_field_embed = False
no_parent_field_type_embed = False
no_parent_state = False

no_input_feed = False
no_copy = False
vocab = None
glove_embed_path = None

train_file = None
dev_file = None

batch_size = 32
dropout = 0.
word_dropout = 0.
decoder_word_dropout = 0.3
primitive_token_label_smoothing = 0.0
src_token_label_smoothing = 0.0

negative_sample_type = best

# training schedule details
valid_metric = acc
valid_every_epoch = 1
log_every = 10

save_to = model
save_all_models = False
patience = 5
max_num_trial = 10
uniform_init = None
glorot_init = True
clip_grad = 5.
max_epoch = -1
optimizer = Adam
lr = 0.0002
lr_decay = 0.
beta_1 = 0.5
lr_decay_after_epoch = 0
decay_lr_every_epoch = False
reset_optimizer = False
eval_top_pred_only = False

#### decoding/validation/testing ####
load_model = None
beam_size = 5
decode_max_time_step = 100
sample_size = 5
test_file = None
save_decode_to = None


[DISCRIMINATOR]
data = discriminator/data/multivac
save = discriminator/checkpoints
expname = test

# model arguments
vocab_size = 0
batch_size = 64
num_epochs = 5
filter_sizes = (10, 5, 4, 3)
num_filters = 20
hidden_dims = 10
dropout_prob1 = 0.5 
dropout_prob2 = 0.8
early_stopping = False

# training arguments
epochs = 15
batchsize = 64
lr = 0.0004
beta_1 = 0.5
wd = 0.
optim = adam
sparse = False
