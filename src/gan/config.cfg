[GAN]
ROLLOUT_NUM = 2

G_STEPS = 1
D_STEPS = 1
K_STEPS = 2

SEED = 12
TOTAL_EPOCHS = 200
GENERATED_NUM = 100

kg_directory = ../../data
glove_dir = ../../models
glove_file = DA_glove_embeddings_300.pkl
glove_lower = True
verbose = True

[GENERATOR]
verbose = True
annot_file = data/query_annots.txt
texts_file = data/query_texts.txt
sample_dir = data/samples
output_dir = ../../models
grammar = None
cuda = False

uniform_init = None
glorot_init = True

#### Model configuration ####
batch_size = 32
dropout = 0.
word_dropout = 0.
primitive_token_label_smoothing = 0.9
lstm = lstm
encoder = lstm

# Embedding sizes
embed_size = 128
action_embed_size = 128
field_embed_size = 64
type_embed_size = 64

# Hidden sizes
hidden_size = 256
# ptrnet_hidden_dim = 32
att_vec_size = 256

# readout layer
no_query_vec_to_action_map = False
readout = non_linear
query_vec_to_action_diff_map = False

# supervised attention
sup_attention = False

# parent information switch for decoder LSTM
no_parent_production_embed = False
no_parent_field_embed = False
no_parent_field_type_embed = False
no_parent_state = False

no_input_feed = False
no_copy = False

# training schedule details
PRE_G_EPOCHS = 50
optimizer = Adam
lr = 0.0001
lr_decay = 0.
beta_1 = 0.5
log_every = 10
clip_grad = 5.

#### decoding/validation/testing ####
beam_size = 5
decode_max_time_step = 100


[DISCRIMINATOR]
device = cpu
cuda = False
verbose = False
data = discriminator/data/multivac
label_smoothing = None

#### Model configuration ####
vocab_size = 0
num_epochs = 5
filter_sizes = (10, 5, 4, 3)
num_filters = 20
hidden_dims = 10
dropout_prob1 = 0.5 
dropout_prob2 = 0.8

# training schedule details
batch_size = 64
optim = adam
lr = 0.0004
beta_1 = 0.5
wd = 0.
