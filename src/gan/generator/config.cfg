
['ARGS']
attention_hidden_dim = 50
batch_size = 10
batch_size = 10 
beam_size = 15
clip_grad = 0
data = ''
data_dir = ''
data_type = 'eng'
decode_max_time_step = 100
decode_max_time_step = 100 
decoder_hidden_dim = 256
dropout = 0.2
enable_copy = True
encoder = 'bilstm'
encoder_hidden_dim =256
frontier_node_type_feed = True
head_nt_constraint = True
max_epoch = 50
max_epoch = 50 
max_query_length = 70
model = None
no_copy = False
no_frontier_node_type_feed = False
no_head_nt_constraint = False
no_parent_action_feed = False
no_parent_hidden_state_feed = False
no_tree_attention = False
node_embed_dim = 256
node_embed_dim = 64 
node_num = 0
optimizer = 'adam'
optimizer = 'adam' 
output_dir = '.outputs'
parent_action_feed = True
parent_hidden_state_feed = True
ptrnet_hidden_dim = 50
random_seed = 181783
rule_embed_dim = 128 
rule_embed_dim = 256
rule_num = 0
save_per_batch = 4000
save_per_batch = 4000 
source_vocab_size = 0
target_vocab_size = 0
train_patience = 10
tree_attention = True
valid_metric = 'bleu'
valid_metric = 'bleu'
valid_per_batch = 4000
valid_per_batch = 4000 
word_embed_dim = 128

# 
# These should probably be controlled at the commandline
# 

# options: ['train','evaluate','decode','interactive']
operation = 'train'

# options: ['self', 'dataset', 'new']
mode = 'self'

type = 'test_data'
saveto = 'decode_results.bin'
input = 'decode_results.bin'
seq2tree_sample_file = 'model.sample'
seq2tree_id_file = 'test.id.txt'
seq2tree_rareword_map = None
seq2seq_decode_file = ''
seq2seq_ref_file = ''
is_nbest = False



