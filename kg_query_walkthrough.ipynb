{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Walk Through for Knowledge Graph Testing\n",
    "This notebook outlines the basic process of submitting queries to MULTIVAC's semantic knowledge graph. \n",
    "First, we set up the required imports and arguments for the test. This process can be performed all at once from the commandline as well:<br><br>\n",
    "`python3 map_queries.py -d data -o models -g models/glove.42B.300d -r model -m transe`<br><br>\n",
    "(threshold and num_top_rel are by default 0.1 and 10 respectively, but can also be set at the commandline with flags `-t` and `-n` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multivac.src.rdf_graph.map_queries import *\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args_dict = {'dir': 'data',\n",
    "             'out': 'models',\n",
    "             'glove': 'models/glove.42B.300d',\n",
    "             'run': 'model',\n",
    "             'model': 'transe',\n",
    "             'threshold': 0.1,\n",
    "             'num_top_rel': 10}\n",
    "\n",
    "args_dict['search'] = '/Users/ben_ryan/Documents/DARPA_ASKE/Phase_II/openke_experiments/query_tester.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we load up the knowledge graph embedding model previously calculated. This embedding model allows us to assign probabilities to missing nodes or relationships in the knowledge graph proposed via submitted queries. Here we are using TransE, an approach which models relationships by interpreting them as translations operating on the\n",
    "low-dimensional embeddings of entities. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "con = config.Config()\n",
    "con.set_in_path(args_dict['dir']+os.path.sep)\n",
    "con.set_work_threads(8)\n",
    "con.set_dimension(100)\n",
    "con.set_test_link_prediction(True)\n",
    "con.set_test_triple_classification(True)\n",
    "\n",
    "files = glob.glob(os.path.join(args_dict['out'],'*tf*'))\n",
    "times = list(set([file.split('.')[2] for file in files]))\n",
    "ifile = max([datetime.strptime(x, '%d%b%Y-%H:%M:%S') for x in times]).strftime('%d%b%Y-%H:%M:%S')\n",
    "con.set_import_files(os.path.join(args_dict['out'], 'model.vec.{}.tf'.format(ifile)))\n",
    "\n",
    "con.init()\n",
    "kem = set_model_choice(args_dict['model'])\n",
    "con.set_model(kem)\n",
    "\n",
    "\n",
    "files = [x for x in os.listdir(con.in_path) if '2id' in x]\n",
    "rel_file = get_newest_file(con.in_path, files, 'relation')\n",
    "ent_file = get_newest_file(con.in_path, files, 'entity')\n",
    "trn_file = get_newest_file(con.in_path, files, 'train')\n",
    "\n",
    "entities = pd.read_csv(ent_file, sep='\\t', \n",
    "                       names=[\"Ent\",\"Id\"], skiprows=1)\n",
    "relations = pd.read_csv(rel_file, sep='\\t', \n",
    "                        names=[\"Rel\",\"Id\"], skiprows=1)\n",
    "train = pd.read_csv(trn_file, sep='\\t', \n",
    "                    names=[\"Head\",\"Tail\",\"Relation\"], skiprows=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then set up our NLP toolset -- a wrapper to the Stanford CoreNLP parsing engine, and a GloVe embedding model. Here we use the large scale, pre-trained GloVe embedding model given the open domain nature of potential submitted questions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> File found, loading to memory\n"
     ]
    }
   ],
   "source": [
    "annots =  \"tokenize ssplit pos depparse natlog openie ner coref\",\n",
    "props  = {\"openie.triple.strict\": \"true\",\n",
    "          \"openie.openie.resolve_coref\": \"true\"}\n",
    "\n",
    "parser = StanfordParser(annots=annots, props=props)\n",
    "\n",
    "glove_vocab, glove_emb = load_word_vectors(args_dict['glove'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we load the query file and parse the queries into their semantic components. These are then matched to the knowledge graph as complete Subject-Relation-Object triples, or as partial triples with imputed portions. Complete triples return one result each, while partials will return up to `num_top_rel` results each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What role do asymptomatically infected individuals play in transmission dynamics?\n",
      "How are power converters traded at Tosche Station?\n"
     ]
    }
   ],
   "source": [
    "queries = pd.read_csv(args_dict['search'])\n",
    "for q in queries.Query:\n",
    "    print(q)\n",
    "    \n",
    "parse = lambda z: stanford_parse(parser, z, sub_rdfs=True).get_rdfs(use_tokens=False, \n",
    "                                                                    how='longest')\n",
    "triples = queries.Query.apply(parse)\n",
    "results = triples.apply(lambda x: get_answers(con, x, \n",
    "                                              glove_vocab, \n",
    "                                              glove_emb, \n",
    "                                              entities, \n",
    "                                              relations,\n",
    "                                              args_dict['num_top_rel'], \n",
    "                                              args_dict['threshold']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('infected individuals', 'should play', 'critical role', 0.1093660369515419)]\n",
      "[('power grids', 'trade', 'station | meteorological station', 0.12902237474918365)]\n"
     ]
    }
   ],
   "source": [
    "for result in results:\n",
    "    print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
