## Query Training
### Confirm Execution of Queries as Models
MULTIVAC will accept properly formatted queries, parse them into compatible formats and match their components to relevant nodes and clusters in its MLN knowledge base to produce answers for those queries. This functionality will be tested and refined in several ways, first using queries auto-extracted from the source texts and then with human expert review. 

MULTIVAC will extract literal expert queries, identified by parsing full texts into component sentences and selecting sentences that end in question marks, as well as deriving expert queries from abstracts using a modified version of deep learning query-generation system QG-Net.<sup>[1](#1)</sup>  QG-Net is a recurrent neural network (RNN)-based model that takes as inputs a “context” (typically a limited amount of natural language text containing relevant information) and an “answer” (a specific fact or figure found within the context) and outputs a question tailored to produce that answer. Our adaptation of QG-Net will be written in Python, in line with the current implementation and the rest of the MULTIVAC system.

MULTIVAC supplies article abstracts from the metadata file saved from the initial datastore acquisition. These abstracts serve as the “context” data, and MULTIVAC will select parts of the abstracts as the “answer” data. These parts will be selected based on two heuristics:
* Words that occur in the same context (sentence or paragraph) as model parameters and descriptions in the main body of the article or that occur in the article title signify phrases that can serve as feasible answers.
* Words with a high TF-IDF score for a particular article signify phrases that can serve as feasible answers.

![alt text](https://github.com/GallupGovt/multivac/blob/master/images/qgnet.png 'QG-net schematic')
<br>Illustration of the QG-net system. Source: https://github.com/moonlightlane/QG-Net

Given context and answer inputs, QG-Net generates different questions that focus on the relevant contextual information that different answers provide. More specifically, it uses MULTIVAC’s domain-adapted GloVe embedding model to represent words as vectors coupled with speech tags, name entity flags and word case tags from the initial parsing as inputs in the context reader to generate diverse context word representations. Finally, a question generator generates the question text word-by-word given all context word representations.

The resulting queries are then applied in order to MULTIVAC’s MLN ontology and the resulting answers are compared with the “answer” extracted directly from the source data. Good matches, in the sense of high scores for semantic completeness, lexical similarity and syntactical similarity will indicate good performance of the ontology. These measures will be assessed using approaches including but not necessarily limited to latent semantic analysis and minimal edit distance metrics. 

This process serves an important quality control purpose, but more importantly this round of query generation allows MULTIVAC to bootstrap a training data set for learning how to generate its own queries without direct reference to previously existing research questions or findings. Samples of this training data will be reviewed and vetted by live human experts as a template for the essential inclusion of human expertise in MULTIVAC’s workflow in eventual production versions of the system. These human experts will also test the trained system with their own novel queries. The involvement of human experts in this phase serves two purposes. First, human expert review of the extracted queries and outputs provides a crucial check on system performance and helps identify areas most in need of improvement or optimization. Second, novel queries submitted by actual human experts provides an “out of sample” test for a system trained on a finite corpus.

### <a name='gan'>Machine Generation of Expert Queries</a>
As a final step in developing its revolutionary machine-assisted inference capabilities, MULTIVAC will train a Generative Adversarial Network (GAN) to produce well-formed, novel expert queries without human intervention. GANs comprise two main components, the generator and the discriminator. The more traditional discriminator network is a standard convolutional neural network that learns the boundaries between classes — for instance, well-formed expert queries and nonsense queries — by training on real-world examples. The generator network is an inverse convolutional network that models the distribution of individual classes in terms of their features. Thus, the generator network generates new query instances, while the discriminator evaluates them for validity.

The discriminator network will be trained on the accrued library of queries generated by MULTIVAC and the human expert participants. Meanwhile, the generator will ingest models, parameters, factors and relationships from the MLN knowledge base and return a “query” constructed from them. The generator network compiles the queries from the formulas in MULTIVAC’s MLN knowledge base using Markov chains to mimic the semantic query grammars embedded there. This novel query is fed to the discriminator along with the existing set of curated expert queries. The discriminator considers both these real and generated queries and assigns probabilities of their authenticity, gradually learning to assign higher probabilities to “authentic” queries and lower ones to inauthentic queries.

<p align='center'><img src="https://github.com/GallupGovt/multivac/blob/master/images/gan.png" alt="GAN Design Graphic" width="600"></p>

GAN architectures are trained dialectically, first training the discriminator on the existing query library, then training the generator against a static discriminator. The discriminator is then trained again, accounting for examples on which it failed, and so on. MULTIVAC’s discriminator will also be augmented by a “real-world” feedback loop; when the generator produces a query, the discriminator scores it, but the query is also submitted against the MLN knowledge base. If it produces results, the query is added to the discriminator training set as a valid expert query, regardless of the initial score given by the discriminator. Thus, new queries and query types can be added to the training library from successful novel queries. In the final iteration, the system will include a hypothesis evaluation loop looking at the explanatory power of a given machine-generated hypothesis and weighting up those that are novel, have a potentially high explanatory power and are plausible in the current context. This GAN implementation will be written in Python leveraging the Keras API with a TensorFlow backend.

### End Notes
- <sup><a name='1'>1</a></sup> Z. Wang, A. S. Lan, W. Nie, P. Grimaldi, R. Schloss, and R. G. Baraniuk, "QG-Net: A Data-Driven Question Generation Model for Educational Content," ACM Conference on Learning at Scale (L@S), pp. 1-10, June 2018. Full text available at https://people.umass.edu/~andrewlan/papers/18l@s-qgen.pdf <br>
