{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is E:\\Users\\Peter_Rasmussen\\gh\\multivac\n",
      "Directory data\\raw already exists\n",
      "Directory data\\interim already exists\n",
      "Directory data\\processed already exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# move current working directory up two levels to root\n",
    "# not pretty but this is a notebook\n",
    "# don't run this cell more than once or you'll move another two directories up, which wouldn't be good\n",
    "os.chdir(os.pardir); os.chdir(os.pardir)\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from src import utilities\n",
    "import settings\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import re\n",
    "import feedparser\n",
    "import time\n",
    "import re\n",
    "\n",
    "# create a .env file in the root directory wp/ if you'd like to use dotenv\n",
    "# .env not included in version control, so credentials can be stored in this file\n",
    "\"\"\"\n",
    "SPRINGER_API_KEY=your_springer_api_key\n",
    "\"\"\"\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# load environment variables from .env\n",
    "# don't print these out in the notebook in the event your changes accidentally get  incorporated into version control\n",
    "springer_api_key = os.environ.get('SPRINGER_API_KEY')\n",
    "ieee_api_key = os.environ.get('IEEE_API_KEY')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_terms(terms):\n",
    "    return '+AND+'.join(['all:' + term for term in terms])\n",
    "\n",
    "\n",
    "def get_total_number_of_results(url, params):\n",
    "    xml_text = requests.get(url, params=params).text\n",
    "    return int(bs(xml_text, 'lxml').find('opensearch:totalresults').contents[0])\n",
    "\n",
    "\n",
    "def query_api(url, terms, params, wait_time=3, verbose=False):\n",
    "    \n",
    "    # get total number of results\n",
    "    n_results = get_total_number_of_results(url, {'start': 0, 'max_results': 1})\n",
    "    if verbose: \n",
    "        print('%s total results, %s second wait time between each call' % (str(n_results), str(wait_time)))\n",
    "    \n",
    "    # build list to iterate over\n",
    "    starts = list(range(0, n_results, params['max_results']))  # start, stop, step\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    # iterate over list to get all results\n",
    "    for ix, start in enumerate(starts):\n",
    "        params_ = copy.deepcopy(params) \n",
    "        params_['start'] = start\n",
    "\n",
    "        # ping api and retrieve xml for all articles in page\n",
    "        xml_text = requests.get(url, params=params_).text\n",
    "\n",
    "        # process xml page feed\n",
    "        page_feed = feedparser.parse(xml_text)\n",
    "        entries = page_feed['entries']\n",
    "        \n",
    "        if ix == 0:\n",
    "            metadata = entries\n",
    "        else:\n",
    "            metadata.extend(entries)\n",
    "        time.sleep(wait_time)\n",
    "    if verbose: print('')\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 total results, 3 second wait time between each call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build query\n",
    "source = 'arxiv'\n",
    "params = {'source': source, 'start': 0, 'max_results': 20, 'sortBy': 'relevance', 'sortOrder': 'descending'}\n",
    "wait_time = 3\n",
    "\n",
    "terms1 = ['susceptible', 'infected', 'recovered']\n",
    "terms2 = ['sir', 'model', 'disease']\n",
    "terms3 = ['irSIR', 'model']\n",
    "q = '%28' + prep_terms(terms1) + '%29' + 'OR' + '%28' + prep_terms(terms2) + '%29'\n",
    "url = 'http://export.arxiv.org/api/query?search_query=' + q\n",
    "arxiv_metadata = query_api(url, terms, params, verbose=True)\n",
    "\n",
    "\n",
    "# url2 = 'http://export.arxiv.org/api/query?search_query=' + prep_terms(terms2)\n",
    "# md2 = query_api(url2, terms2, params, verbose=True)\n",
    "\n",
    "# md = m1 + md2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "424\n"
     ]
    }
   ],
   "source": [
    "li = ['sir model', 'susceptible infected recovered', 'irSIR model', 'SIS epidemic model', 'susceptible-exposed-infected']\n",
    "make_q = lambda li: '(' + ' OR '.join(['\"' + s + '\"' for s in li]) + ')'\n",
    "q = make_q(li)\n",
    "\n",
    "base = 'http://api.springernature.com/openaccess/json?q='\n",
    "url = base + q\n",
    "params = {'source': 'springer', 'openaccess': 'true', 'api_key': springer_api_key, 'p': 20, 's': 1}\n",
    "params_ = copy.deepcopy(params)\n",
    "r = requests.get(url, params_)\n",
    "springer_metadata = []\n",
    "\n",
    "while True:\n",
    "    r = requests.get(url, params_)\n",
    "    if len(r.json()['records']) == 0:\n",
    "        break\n",
    "    params_['s'] = params_['s'] + params_['p']\n",
    "    springer_metadata += r.json()['records']\n",
    "    time.sleep(wait_time)\n",
    "print(len(springer_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE Xplore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n"
     ]
    }
   ],
   "source": [
    "base = 'http://ieeexploreapi.ieee.org/api/v1/search/articles?'\n",
    "url = base + q\n",
    "params = {'max_records': 20, 'start_record': 1, 'querytext': q, 'apikey': ieee_api_key}\n",
    "params_ = copy.deepcopy(params)\n",
    "\n",
    "ieee_metadata = []\n",
    "while True:\n",
    "    r = requests.get(url, params_)\n",
    "    if params_['start_record'] > r.json()['total_records']:\n",
    "        break\n",
    "    for article in r.json()['articles']:\n",
    "        if i['access_type'] != 'LOCKED':\n",
    "            ieee_metadata.append(article)\n",
    "    params_['start_record'] = params_['start_record'] + params_['max_records']\n",
    "    time.sleep(wait_time)\n",
    "print(len(ieee_metadata))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed (Entrez)\n",
    "* https://marcobonzanini.com/2015/01/12/searching-pubmed-with-python/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def process_article_metadata(metadata):\n",
    "#     \"\"\"Extract metadata for one article and organize metadata into a dictionary.\n",
    "#     Inputs:\n",
    "#         metadata    String; Read in from API call\n",
    "#     Outputs:\n",
    "#         d  Dictionary of article metadata\n",
    "#     \"\"\"\n",
    "\n",
    "#     # for each article extract and organize metadata\n",
    "#     metadata = bs(metadata.strip(), 'lxml')\n",
    "\n",
    "#     # each article's metadata contained in a dictionary\n",
    "#     d = {}\n",
    "\n",
    "#     # add single-value attributes\n",
    "#     single_value_attributes = ['id', 'updated', 'published', 'title', 'summary', 'doi']\n",
    "#     for single_value_attribute in single_value_attributes:\n",
    "#         try:\n",
    "#             v = metadata.find(single_value_attribute).contents[0].strip()\n",
    "#         except AttributeError:\n",
    "#             # some articles don't have doi numbers so fall back on arxiv doi\n",
    "#             if single_value_attribute=='doi':\n",
    "#                 v = d['id']\n",
    "#             else:\n",
    "#                 v = None\n",
    "#         d[single_value_attribute] = v\n",
    "\n",
    "#     # add multiple-value attributes and edge-case single-value attributes\n",
    "#     d['arxiv:primary_category'] = metadata.find('arxiv:primary_category').attrs['term']\n",
    "#     d['arxiv_categories'] = [x['term'] for x in metadata.find_all('category')]\n",
    "#     d_links = {}\n",
    "#     for link in metadata.find_all('link'):\n",
    "#         if 'title' in link.attrs:\n",
    "#             k, v = link.attrs['title'], link.attrs['href']\n",
    "#             d_links[k] = v\n",
    "#     d['links'] = d_links\n",
    "#     d['authors'] = [x.contents[0] for x in metadata.find_all('name')]\n",
    "\n",
    "#     return d\n",
    "\n",
    "\n",
    "# def get_metadata_from_page(xml_text):\n",
    "#     \"\"\"\n",
    "#     Usage of output:\n",
    "#         As an OrderedDict, d_page_metadata has the usual dictionary functionality\n",
    "#         It can also be accessed like a list using the approach below (Python 3 approach below):\n",
    "#             items = list(d_page_metadata.items())\n",
    "#             items[0]\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # prep metadata returned by api query\n",
    "#     articles_metadata = re.sub(' +', ' ', xml_text.replace('arxiv:doi', 'doi').replace('\\n', ' ')).strip().split('<entry>')[1:]\n",
    "\n",
    "#     # iterate over each article and extract and organize metadata\n",
    "#     d_page_metadata = OrderedDict()\n",
    "#     for article_metadata in articles_metadata:\n",
    "#         v = copy.deepcopy(process_article_metadata(article_metadata))\n",
    "#         k = v.pop('doi')\n",
    "#         d_page_metadata[k] = v\n",
    "    \n",
    "#     return d_page_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps2ascii, gzip\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(raw_src, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = r'E:\\Users\\Peter_Rasmussen\\gh\\multivac\\data\\raw\\arxiv'\n",
    "raw_src = os.path.join(raw_dir, '1411.2370v2.ps.gz')\n",
    "# with gzip.open(raw_src, 'rb') as f:\n",
    "#     ps = f.read()\n",
    "    \n",
    "def opener(filename):\n",
    "    f = open(filename,'rb')\n",
    "    if (f.read(2) == '\\x1f\\x8b'):\n",
    "        f.seek(0)\n",
    "        return gzip.GzipFile(fileobj=f)\n",
    "    else:\n",
    "        f.seek(0)\n",
    "        return f\n",
    "f = opener(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with opener(raw_src) as f:\n",
    "    ps = f.read().decode('iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d_metadata.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d_metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(80 * '*')\n",
    "print(xml_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for link in article_metadata.find_all('link'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'author' in link.attrs:\n",
    "    k, v = link.attrs['title'], link.attrs['href']\n",
    "    d_links[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_metadata.find_all('link')[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = ''.join(soup.find('summary').contents).strip().replace('\\n', ' ')\n",
    "title = ''.join(soup.find('title').contents).strip().replace('\\n', ' ')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup.find_all('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = xml.etree.ElementTree.parse('xml.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.findall('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.data(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "base = 'http://export.arxiv.org/api/'\n",
    "method = 'query'\n",
    "search_term = 'electron'\n",
    "parameters\n",
    "parameters = 'search_query=all:' + electron&start=0&max_results=10'\n",
    "query = base + method + '?' + parameters\n",
    "data = urllib.request.urlopen(query).read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
