{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is E:\\Users\\Peter_Rasmussen\\gh\n",
      "Directory multivac\\data already exists\n",
      "Directory multivac\\data\\raw already exists\n",
      "Directory multivac\\data\\interim already exists\n",
      "Directory multivac\\data\\processed already exists\n",
      "Directory multivac\\data\\processed\\metadata already exists\n",
      "Directory multivac\\models already exists\n",
      "Directory multivac\\data\\raw\\arxiv already exists\n",
      "Directory multivac\\data\\raw\\pubmed already exists\n",
      "Directory multivac\\data\\raw\\springer already exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# move current working directory up two levels to root\n",
    "# not pretty but this is a notebook\n",
    "# don't run this cell more than once or you'll move another two directories up, which wouldn't be good\n",
    "os.chdir(os.pardir); os.chdir(os.pardir); os.chdir(os.pardir)\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from multivac.src import utilities\n",
    "from multivac import settings\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import re\n",
    "import feedparser\n",
    "import pubmed_parser\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "# create a .env file in the root directory wp/ if you'd like to use dotenv\n",
    "# .env not included in version control, so credentials can be stored in this file\n",
    "\"\"\"\n",
    "SPRINGER_API_KEY=your_springer_api_key\n",
    "\"\"\"\n",
    "\n",
    "env_path = Path('.') / 'multivac' / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# load environment variables from .env\n",
    "# don't print these out in the notebook in the event your changes accidentally get incorporated into version control\n",
    "springer_api_key = os.environ.get('SPRINGER_API_KEY')\n",
    "# ieee_api_key = os.environ.get('IEEE_API_KEY')\n",
    "user_email = os.environ.get('USER_EMAIL')  # courtesy to NIH to include your email\n",
    "\n",
    "wait_time = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_terms(terms):\n",
    "    \"\"\"Format search terms to be compatible with Arxiv API query.\"\"\"\n",
    "    return '+AND+'.join(['all:' + term for term in terms])\n",
    "\n",
    "\n",
    "def get_total_number_of_results(url, params):\n",
    "    \"\"\"Retrieve total number of results from Arxiv API query.\"\"\"\n",
    "    xml_text = requests.get(url, params=params).text\n",
    "    return int(bs(xml_text, 'lxml').find('opensearch:totalresults').contents[0])\n",
    "\n",
    "\n",
    "def query_api(url, terms, params, wait_time=3, verbose=False):\n",
    "    \"\"\"Query Arxiv API to obtain metadata and lookup URLs of queried articles.\"\"\"\n",
    "    # get total number of results\n",
    "    n_results = get_total_number_of_results(url, {'start': 0, 'max_results': 1})\n",
    "    if verbose: \n",
    "        print('%s total results, %s second wait time between each call' % (str(n_results), str(wait_time)))\n",
    "    \n",
    "    # build list to iterate over\n",
    "    starts = list(range(0, n_results, params['max_results']))  # start, stop, step\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    # iterate over list to get all results\n",
    "    for ix, start in enumerate(starts):\n",
    "        if verbose:\n",
    "            print('*', end='')\n",
    "        params_ = copy.deepcopy(params) \n",
    "        params_['start'] = start\n",
    "\n",
    "        # ping api and retrieve xml for all articles in page\n",
    "        xml_text = requests.get(url, params=params_).text\n",
    "\n",
    "        # process xml page feed \n",
    "        page_feed = feedparser.parse(xml_text)\n",
    "        entries = page_feed['entries']\n",
    "        \n",
    "        if ix == 0:\n",
    "            metadata = entries\n",
    "        else:\n",
    "            metadata.extend(entries)\n",
    "        time.sleep(wait_time)\n",
    "    if verbose: print('')\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "940 total results, 1 second wait time between each call\n",
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build query and get metadata of articles matching our search criteria\n",
    "params = {'start': 0, 'max_results': 100, 'sortBy': 'relevance', 'sortOrder': 'descending'}\n",
    "li = [x.replace('-', ' ').split(' ') for x in settings.terms]\n",
    "q = 'OR'.join(['%28' + prep_terms(x) + '%29' for x in li])\n",
    "url = 'http://export.arxiv.org/api/query?search_query=' + q\n",
    "arxiv_metadata = query_api(url, q, params, wait_time=1, verbose=True)\n",
    "\n",
    "# save pdfs of articles that matched our search criteria\n",
    "# we use doi as the filename when that id is present; otherwise we use the arxiv id\n",
    "for ix, md in enumerate(arxiv_metadata):\n",
    "    url = md['id']\n",
    "    pdf_url = url.replace('/abs/', '/pdf/')\n",
    "    article_fn = url.split('/abs/')[-1]\n",
    "    article_fn = '_'.join(article_fn.split('/')) + '.pdf'\n",
    "    arxiv_metadata[ix]['fn'] = article_fn  # specify filename so we can associate each pdf with its metadata down the road\n",
    "    dst = settings.raw_dir / 'arxiv' / article_fn\n",
    "    if not os.path.exists(dst):\n",
    "        r = requests.get(pdf_url)\n",
    "        with open(dst, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        time.sleep(0.3)\n",
    "\n",
    "# save arxiv metadata\n",
    "fn = 'arxiv' + '.pkl'\n",
    "dst = settings.metadata_dir / fn\n",
    "with open(dst, 'wb') as f:\n",
    "    pickle.dump(arxiv_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "573 total Springer articles\n"
     ]
    }
   ],
   "source": [
    "# build query to retrieve metadata\n",
    "make_q = lambda li: '(' + ' OR '.join(['\"' + s + '\"' for s in li]) + ')'\n",
    "q = make_q(settings.terms)\n",
    "base = 'http://api.springernature.com/openaccess/json?q='\n",
    "url = base + q\n",
    "params = {'source': 'springer', 'openaccess': 'true', 'api_key': springer_api_key, 'p': 20, 's': 1}\n",
    "params_ = copy.deepcopy(params)\n",
    "\n",
    "# retrieve metadata\n",
    "springer_metadata = []\n",
    "while True:\n",
    "    r = requests.get(url, params_)\n",
    "    if len(r.json()['records']) == 0:\n",
    "        break\n",
    "    params_['s'] = params_['s'] + params_['p']\n",
    "    springer_metadata += r.json()['records']\n",
    "    time.sleep(wait_time)\n",
    "print('%s total Springer articles' % len(springer_metadata))\n",
    "\n",
    "# iterate over springer metadata and download html for each article\n",
    "waits = (2**x for x in range(0,6))  # we use a generator to increase wait times with each connection error\n",
    "for ix, md in enumerate(springer_metadata):\n",
    "    fn = md['doi'].replace('/', '-')\n",
    "    if len(fn) == 0:\n",
    "        fn = md['identifier']\n",
    "    fn = fn + '.html'\n",
    "    springer_metadata[ix]['fn'] = fn\n",
    "    dst = settings.raw_dir / 'springer' / fn\n",
    "    if not os.path.exists(dst):\n",
    "        try:\n",
    "            r = requests.get(md['url'][0]['value'])\n",
    "        except ConnectionError:\n",
    "            time.sleep(waits.__next__)\n",
    "            r = requests.get(md['url'][0]['value'])\n",
    "        html = bs(r.text).encode('utf-8').decode('utf-8')\n",
    "        with open(dst, 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        time.sleep(3)\n",
    "\n",
    "# save springer metadata\n",
    "dst = settings.metadata_dir / 'springer.pkl'\n",
    "with open(dst, 'wb') as f:\n",
    "    pickle.dump(springer_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed Central (Entrez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search pubmed central for free full text articles containing selected query\n",
    "\n",
    "# get the ids which we then use to get the xml text data\n",
    "replace = lambda s: s.replace(' ', '+')\n",
    "quote = lambda s: '%22' + s + '%22'\n",
    "terms = [quote(replace(s)) for s in settings.terms]\n",
    "term = 'term='+ '%28'+ '+OR+'.join(terms) + '%29'\n",
    "fulltext = 'free+fulltext%5bfilter%5d'\n",
    "retmax = 'retmax=' + str(2000)\n",
    "base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc'\n",
    "url = base + '&' + term + '+' + fulltext + '&' + retmax + '&' + user_email\n",
    "r = requests.get(url)\n",
    "ids = [x.contents[0] for x in bs(r.text).find_all('id')]\n",
    "\n",
    "print('%s Pubmed Central (PMC) articles' % ids)\n",
    "\n",
    "# get xml text data and save to disk\n",
    "for i in ids:\n",
    "    pmc_id = 'pmc' + str(i)\n",
    "    fn = (pmc_id + '.xml')\n",
    "    dst = settings.raw_dir / 'pubmed' / fn\n",
    "    if not os.path.exists(dst):\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=' + str(i)\n",
    "        r = requests.get(url, params={'id': i})\n",
    "        xml = r.text\n",
    "        with open(dst, 'w') as f:\n",
    "            f.write(xml)\n",
    "        time.sleep(0.5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
