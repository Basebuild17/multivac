{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current working directory is E:\\Users\\Peter_Rasmussen\\gh\\multivac\n",
      "Directory data\\raw already exists\n",
      "Directory data\\interim already exists\n",
      "Directory data\\processed already exists\n",
      "Directory data\\raw\\arxiv already exists\n",
      "Directory data\\raw\\pubmed already exists\n",
      "Directory data\\raw\\springer already exists\n"
     ]
    }
   ],
   "source": [
    "%load_ext dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# move current working directory up two levels to root\n",
    "# not pretty but this is a notebook\n",
    "# don't run this cell more than once or you'll move another two directories up, which wouldn't be good\n",
    "os.chdir(os.pardir); os.chdir(os.pardir)\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from src import utilities\n",
    "import settings\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import re\n",
    "import feedparser\n",
    "import pubmed_parser\n",
    "import time\n",
    "import re\n",
    "import pickle\n",
    "\n",
    "\n",
    "# create a .env file in the root directory wp/ if you'd like to use dotenv\n",
    "# .env not included in version control, so credentials can be stored in this file\n",
    "\"\"\"\n",
    "SPRINGER_API_KEY=your_springer_api_key\n",
    "\"\"\"\n",
    "\n",
    "env_path = Path('.') / '.env'\n",
    "load_dotenv(env_path)\n",
    "\n",
    "# load environment variables from .env\n",
    "# don't print these out in the notebook in the event your changes accidentally get  incorporated into version control\n",
    "springer_api_key = os.environ.get('SPRINGER_API_KEY')\n",
    "ieee_api_key = os.environ.get('IEEE_API_KEY')\n",
    "user_email = os.environ.get('USER_EMAIL')  # courtesy to NIH to include your email\n",
    "\n",
    "wait_time = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arxiv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_terms(terms):\n",
    "    return '+AND+'.join(['all:' + term for term in terms])\n",
    "\n",
    "\n",
    "def get_total_number_of_results(url, params):\n",
    "    xml_text = requests.get(url, params=params).text\n",
    "    return int(bs(xml_text, 'lxml').find('opensearch:totalresults').contents[0])\n",
    "\n",
    "\n",
    "def query_api(url, terms, params, wait_time=3, verbose=False):\n",
    "    \n",
    "    # get total number of results\n",
    "    n_results = get_total_number_of_results(url, {'start': 0, 'max_results': 1})\n",
    "    if verbose: \n",
    "        print('%s total results, %s second wait time between each call' % (str(n_results), str(wait_time)))\n",
    "    \n",
    "    # build list to iterate over\n",
    "    starts = list(range(0, n_results, params['max_results']))  # start, stop, step\n",
    "\n",
    "    metadata = []\n",
    "\n",
    "    # iterate over list to get all results\n",
    "    for ix, start in enumerate(starts):\n",
    "        params_ = copy.deepcopy(params) \n",
    "        params_['start'] = start\n",
    "\n",
    "        # ping api and retrieve xml for all articles in page\n",
    "        xml_text = requests.get(url, params=params_).text\n",
    "\n",
    "        # process xml page feed \n",
    "        page_feed = feedparser.parse(xml_text)\n",
    "        entries = page_feed['entries']\n",
    "        \n",
    "        if ix == 0:\n",
    "            metadata = entries\n",
    "        else:\n",
    "            metadata.extend(entries)\n",
    "        time.sleep(wait_time)\n",
    "    if verbose: print('')\n",
    "    return metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 total results, 3 second wait time between each call\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# build query and get metadata of articles matching our search criteria\n",
    "params = {'start': 0, 'max_results': 20, 'sortBy': 'relevance', 'sortOrder': 'descending'}\n",
    "li = [x.replace('-', ' ').split(' ') for x in settings.terms]\n",
    "q = 'OR'.join(['%28' + prep_terms(x) + '%29' for x in li])\n",
    "url = 'http://export.arxiv.org/api/query?search_query=' + q\n",
    "arxiv_metadata = query_api(url, q, params, verbose=True)\n",
    "\n",
    "# save pdfs of articles that matched our search criteria\n",
    "# we use doi as the filename when that id is present; otherwise we use the arxiv id\n",
    "for ix, md in enumerate(arxiv_metadata):\n",
    "    url = md['id']\n",
    "    pdf_url = url.replace('/abs/', '/pdf/')\n",
    "    fn = url.split('/abs/')[-1]\n",
    "    fn = '_'.join(fn.split('/')) + '.pdf'\n",
    "    arxiv_metadata[ix]['fn'] = fn  # specify filename so we can associate each pdf with its metadata down the road\n",
    "    dst = settings.arxiv_dir / fn\n",
    "    if not os.path.exists(dst):\n",
    "        r = requests.get(pdf_url)\n",
    "        with open(dst, 'wb') as f:\n",
    "            f.write(r.content)\n",
    "        time.sleep(0.3)\n",
    "        \n",
    "# save arxiv metadata\n",
    "dst = settings.metadata_dir / 'arxiv.pkl'\n",
    "with open(dst, 'wb') as f:\n",
    "    pickle.dump(arxiv_metadata, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Springer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'10.1038/s41598-018-36116-6'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "springer_metadata[0]['doi']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "382\n"
     ]
    }
   ],
   "source": [
    "# build query to retrieve metadata\n",
    "li = ['sir model', 'susceptible infected recovered', 'irSIR model']\n",
    "make_q = lambda li: '(' + ' OR '.join(['\"' + s + '\"' for s in li]) + ')'\n",
    "q = make_q(settings.terms)\n",
    "base = 'http://api.springernature.com/openaccess/json?q='\n",
    "url = base + q\n",
    "params = {'source': 'springer', 'openaccess': 'true', 'api_key': springer_api_key, 'p': 20, 's': 1}\n",
    "params_ = copy.deepcopy(params)\n",
    "# r = requests.get(url, params_)\n",
    "\n",
    "# retrieve metadata\n",
    "springer_metadata = []\n",
    "while True:\n",
    "    r = requests.get(url, params_)\n",
    "    if len(r.json()['records']) == 0:\n",
    "        break\n",
    "    params_['s'] = params_['s'] + params_['p']\n",
    "    springer_metadata += r.json()['records']\n",
    "    time.sleep(wait_time)\n",
    "print(len(springer_metadata))\n",
    "\n",
    "# iterate over springer metadata and download html for each article\n",
    "waits = (2**x for x in range(0,6))  # we use a generator to increase wait times with each connection error\n",
    "for ix, md in enumerate(springer_metadata):\n",
    "    fn = md['doi'].replace('/', '-')\n",
    "    if len(fn) == 0:\n",
    "        fn = md['identifier']\n",
    "    fn = fn + '.html'\n",
    "    springer_metadata[ix]['fn'] = fn\n",
    "    dst = settings.springer_dir / fn\n",
    "    if not os.path.exists(dst):\n",
    "        try:\n",
    "            r = requests.get(md['url'][0]['value'])\n",
    "        except ConnectionError:\n",
    "            time.sleep(waits.__next__)\n",
    "            r = requests.get(md['url'][0]['value'])\n",
    "        html = bs(r.text).encode('utf-8').decode('utf-8')\n",
    "        with open(dst, 'w', encoding='utf-8') as f:\n",
    "            f.write(html)\n",
    "        time.sleep(3)\n",
    "\n",
    "# save springer metadata\n",
    "dst = settings.metadata_dir / 'springer.pkl'\n",
    "with open(dst, 'wb') as f:\n",
    "    pickle.dump(springer_metadata, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'contentType': 'Article',\n",
       " 'identifier': 'doi:10.1038/s41598-018-36116-6',\n",
       " 'url': [{'format': '',\n",
       "   'platform': '',\n",
       "   'value': 'http://dx.doi.org/10.1038/s41598-018-36116-6'}],\n",
       " 'title': 'How Physical Proximity Shapes Complex Social Networks',\n",
       " 'creators': [{'creator': 'Stopczynski, Arkadiusz'},\n",
       "  {'creator': 'Pentland, Alex ‘Sandy’'},\n",
       "  {'creator': 'Lehmann, Sune'}],\n",
       " 'publicationName': 'Scientific Reports',\n",
       " 'issn': '',\n",
       " 'eissn': '2045-2322',\n",
       " 'openaccess': 'true',\n",
       " 'journalid': '41598',\n",
       " 'doi': '10.1038/s41598-018-36116-6',\n",
       " 'publisher': 'Nature',\n",
       " 'publicationDate': '2018-12-07',\n",
       " 'onlineDate': '2018-12-07',\n",
       " 'coverDate': '2018-12',\n",
       " 'printDate': '',\n",
       " 'volume': '8',\n",
       " 'number': '1',\n",
       " 'issuetype': 'Regular',\n",
       " 'topicalCollection': '',\n",
       " 'startingPage': '1',\n",
       " 'endingPage': '10',\n",
       " 'copyright': '©2018 The Author(s)',\n",
       " 'genre': 'OriginalPaper',\n",
       " 'abstract': 'AbstractSocial interactions among humans create complex networks and – despite a recent increase of online communication – the interactions mediated through physical proximity remain a fundamental way for people to connect. A common way to quantify the nature of the links between individuals is to consider repeated interactions: frequently occurring interactions indicate strong ties, such as friendships, while ties with low weights can indicate random encounters. Here we focus on a different dimension: rather than the strength of links, we study physical distance between individuals when a link is activated. The findings presented here are based on a dataset of proximity events in a population of approximately 500 individuals. To quantify the impact of the physical proximity on the dynamic network, we use a simulated epidemic spreading processes in two distinct networks of physical proximity. We consider the network of short - range interactions defined as d \\u2009 $${\\\\boldsymbol{\\\\lesssim }}$$ ≲ \\u20091 meter, and the long - range which includes all interactions d \\u2009 $${\\\\boldsymbol{\\\\lesssim }}$$ ≲ \\u200910 meters. Since these two networks arise from the same set of underlying behavioral data, we are able to quantitatively measure how the specific definition of the proximity network – short-range versus long-range – impacts the resulting network structure as well as spreading dynamics in epidemic simulations. We find that the short-range network – consistent with the literature – is characterized by densely-connected neighborhoods bridged by weak ties. More surprisingly, however, we show that spreading in the long-range network is quite different, mainly shaped by spurious interactions.',\n",
       " 'fn': '10.1038-s41598-018-36116-6.html'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "springer_metadata[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "soup = bs(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "li = [x.contents[0] for x in soup.find('article').find_all('p', {'class':'Para'})]\n",
    "li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# print(soup.find('article').prettify())\n",
    "li = [x.get_text() for x in soup.find('article').find_all('p')]\n",
    "s = 'Springer Nature remains neutral '\n",
    "li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pubmed Central (Entrez)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# search pubmed central for free full text articles containing selected query\n",
    "\n",
    "# get the ids which we then use to get the xml text data\n",
    "replace = lambda s: s.replace(' ', '+')\n",
    "quote = lambda s: '%22' + s + '%22'\n",
    "terms = [quote(replace(s)) for s in settings.terms]\n",
    "term = 'term='+ '%28'+ '+OR+'.join(terms) + '%29'\n",
    "fulltext = 'free+fulltext%5bfilter%5d'\n",
    "retmax = 'retmax=2000'\n",
    "base = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/esearch.fcgi?db=pmc'\n",
    "params = {'retmax': 2000, 'email': user_email}\n",
    "url = base + '&' + term + '+' + fulltext + '&' + retmax\n",
    "r = requests.get(url)\n",
    "ids = [x.contents[0] for x in bs(r.text).find_all('id')]\n",
    "\n",
    "# get xml text data and save to disk\n",
    "for i in ids:\n",
    "    pmc_id = 'pmc' + str(i)\n",
    "    fn = (pmc_id + '.xml')\n",
    "    dst = settings.pubmed_dir / fn\n",
    "    if not os.path.exists(dst):\n",
    "        url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=' + str(i)\n",
    "        r = requests.get(url, params={'id': i})\n",
    "        xml = r.text\n",
    "        with open(dst, 'w') as f:\n",
    "            f.write(xml)\n",
    "        time.sleep(0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STOP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IEEE Xplore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base = 'http://ieeexploreapi.ieee.org/api/v1/search/articles?'\n",
    "# url = base + q\n",
    "# params = {'max_records': 20, 'start_record': 1, 'querytext': q, 'apikey': ieee_api_key}\n",
    "# params_ = copy.deepcopy(params)\n",
    "\n",
    "# ieee_metadata = []\n",
    "# while True:\n",
    "#     r = requests.get(url, params_)\n",
    "#     if params_['start_record'] > r.json()['total_records']:\n",
    "#         break\n",
    "#     for article in r.json()['articles']:\n",
    "#         if i['access_type'] != 'LOCKED':\n",
    "#             ieee_metadata.append(article)\n",
    "#     params_['start_record'] = params_['start_record'] + params_['max_records']\n",
    "#     time.sleep(wait_time)\n",
    "# print(len(ieee_metadata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if len(text) > 0:\n",
    "    pmc_articles[doi] = {'metadata': metadata, 'text': text}\n",
    "\n",
    "text = pubmed_parser.parse_pubmed_paragraph(str(path.absolute()), all_paragraph=True)\n",
    "metadata = pubmed_parser.parse_pubmed_xml(str(path.absolute()))\n",
    "doi = metadata.pop('doi')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_articles[doi]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pmc_articles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pmc_articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paragraphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# r = requests.get(url, params={'id': i})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = r'E:\\Users\\Peter_Rasmussen\\gh\\multivac\\data\\raw\\pubmed\\pmc4760143'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the parsing answer: https://github.com/titipata/pubmed_parser\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = 'https://eutils.ncbi.nlm.nih.gov/entrez/eutils/efetch.fcgi?db=pmc&id=6214536'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pubmed_parser.parse_xml_web(ids[28], save_xml=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xml = r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pubmed_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicts_out = pp.parse_pubmed_paragraph('data/6605965a.nxml', all_paragraph=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xml.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_file_list_ = pd.read_csv('ftp://ftp.ncbi.nlm.nih.gov/pub/pmc/oa_file_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oa_file_list_['Article Citation'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tbd\n",
    "\n",
    "url = 'https://www.ncbi.nlm.nih.gov/pmc/oai/oai.cgi?verb=ListRecords&metadataPrefix=pmc&term=%22sir+model%22+OR%22susceptible+-+infected+-+recovered%22&set=pmc-open'\n",
    "r = requests.get(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text[:1000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# def process_article_metadata(metadata):\n",
    "#     \"\"\"Extract metadata for one article and organize metadata into a dictionary.\n",
    "#     Inputs:\n",
    "#         metadata    String; Read in from API call\n",
    "#     Outputs:\n",
    "#         d  Dictionary of article metadata\n",
    "#     \"\"\"\n",
    "\n",
    "#     # for each article extract and organize metadata\n",
    "#     metadata = bs(metadata.strip(), 'lxml')\n",
    "\n",
    "#     # each article's metadata contained in a dictionary\n",
    "#     d = {}\n",
    "\n",
    "#     # add single-value attributes\n",
    "#     single_value_attributes = ['id', 'updated', 'published', 'title', 'summary', 'doi']\n",
    "#     for single_value_attribute in single_value_attributes:\n",
    "#         try:\n",
    "#             v = metadata.find(single_value_attribute).contents[0].strip()\n",
    "#         except AttributeError:\n",
    "#             # some articles don't have doi numbers so fall back on arxiv doi\n",
    "#             if single_value_attribute=='doi':\n",
    "#                 v = d['id']\n",
    "#             else:\n",
    "#                 v = None\n",
    "#         d[single_value_attribute] = v\n",
    "\n",
    "#     # add multiple-value attributes and edge-case single-value attributes\n",
    "#     d['arxiv:primary_category'] = metadata.find('arxiv:primary_category').attrs['term']\n",
    "#     d['arxiv_categories'] = [x['term'] for x in metadata.find_all('category')]\n",
    "#     d_links = {}\n",
    "#     for link in metadata.find_all('link'):\n",
    "#         if 'title' in link.attrs:\n",
    "#             k, v = link.attrs['title'], link.attrs['href']\n",
    "#             d_links[k] = v\n",
    "#     d['links'] = d_links\n",
    "#     d['authors'] = [x.contents[0] for x in metadata.find_all('name')]\n",
    "\n",
    "#     return d\n",
    "\n",
    "\n",
    "# def get_metadata_from_page(xml_text):\n",
    "#     \"\"\"\n",
    "#     Usage of output:\n",
    "#         As an OrderedDict, d_page_metadata has the usual dictionary functionality\n",
    "#         It can also be accessed like a list using the approach below (Python 3 approach below):\n",
    "#             items = list(d_page_metadata.items())\n",
    "#             items[0]\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # prep metadata returned by api query\n",
    "#     articles_metadata = re.sub(' +', ' ', xml_text.replace('arxiv:doi', 'doi').replace('\\n', ' ')).strip().split('<entry>')[1:]\n",
    "\n",
    "#     # iterate over each article and extract and organize metadata\n",
    "#     d_page_metadata = OrderedDict()\n",
    "#     for article_metadata in articles_metadata:\n",
    "#         v = copy.deepcopy(process_article_metadata(article_metadata))\n",
    "#         k = v.pop('doi')\n",
    "#         d_page_metadata[k] = v\n",
    "    \n",
    "#     return d_page_metadata\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ps2ascii, gzip\n",
    "import gzip\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = gzip.open(raw_src, 'rb')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_dir = r'E:\\Users\\Peter_Rasmussen\\gh\\multivac\\data\\raw\\arxiv'\n",
    "raw_src = os.path.join(raw_dir, '1411.2370v2.ps.gz')\n",
    "# with gzip.open(raw_src, 'rb') as f:\n",
    "#     ps = f.read()\n",
    "    \n",
    "def opener(filename):\n",
    "    f = open(filename,'rb')\n",
    "    if (f.read(2) == '\\x1f\\x8b'):\n",
    "        f.seek(0)\n",
    "        return gzip.GzipFile(fileobj=f)\n",
    "    else:\n",
    "        f.seek(0)\n",
    "        return f\n",
    "f = opener(raw_src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ps = f.read()\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with opener(raw_src) as f:\n",
    "    ps = f.read().decode('iso-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(d_metadata.items())[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(d_metadata.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "next(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print()\n",
    "print(80 * '*')\n",
    "print(xml_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "for link in article_metadata.find_all('link'):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if 'author' in link.attrs:\n",
    "    k, v = link.attrs['title'], link.attrs['href']\n",
    "    d_links[k] = v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "link.attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "article_metadata.find_all('link')[0].attrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "summary = ''.join(soup.find('summary').contents).strip().replace('\\n', ' ')\n",
    "title = ''.join(soup.find('title').contents).strip().replace('\\n', ' ')\n",
    "title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "soup.find_all('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'http://export.arxiv.org/api/query?search_query=all:electron&start=0&max_results=10'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e = xml.etree.ElementTree.parse('xml.xml')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "e.findall('title')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bf.data(r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "\n",
    "base = 'http://export.arxiv.org/api/'\n",
    "method = 'query'\n",
    "search_term = 'electron'\n",
    "parameters\n",
    "parameters = 'search_query=all:' + electron&start=0&max_results=10'\n",
    "query = base + method + '?' + parameters\n",
    "data = urllib.request.urlopen(query).read()\n",
    "print(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
