{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parsing\n",
    "\n",
    "#### This script parses scraped data to retrieve dependencies, POS and lemmatized tokens. \n",
    "\n",
    "Spacy is required for this. Use the following commands to download Spacy:\n",
    "```\n",
    "conda install -c conda-forge spacy\n",
    "\n",
    "python -m spacy download en\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import required packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import spacy\n",
    "import pickle\n",
    "from spacy import displacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for loading data and creating output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(picklePath = None):\n",
    "    \"\"\"Load data - if picklePath is specified, load the pickle. Else, try json file\n",
    "    \"\"\"\n",
    "    if picklePath is not None:\n",
    "        l_docs = pickle.load(open(picklePath, \"rb\" ))\n",
    "    else:\n",
    "\n",
    "        nlp = spacy.load('en')\n",
    "        #if above doesn't work, load english model from local \n",
    "        #nlp = spacy.load('E:/Users/nasser_qadri/AppData/Local/conda/conda/envs/multivac/Lib/site-packages/en_core_web_sm/en_core_web_sm-2.0.0')\n",
    "\n",
    "        #Read JSON data into the datastore variable - this comes from Peter and Domonique's effort. Don\n",
    "        with open('../../data/20181212.json', 'r') as f:\n",
    "            datastore = json.load(f)\n",
    "\n",
    "        ## Create nlpified object\n",
    "        l_docs = [nlp(value['text']) for key,value in list(datastore.items())[0:100] if value['text']]\n",
    "\n",
    "        ## Save pickle of nlpified \n",
    "        with open('NLPifiedDocs-first100.pkl', 'wb') as f:\n",
    "            pickle.dump(l_docs, f)\n",
    "\n",
    "    print('# of documents: ', len(l_docs))\n",
    "    return l_docs\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "def retrieve_JSON_output(l_docs):\n",
    "    \"\"\"Create a JSON output of dependency trees \n",
    "    \n",
    "    This has been replaced with different function\n",
    "    \"\"\"\n",
    "    \n",
    "    sentences = []\n",
    "    dependencyDocuments = []\n",
    "    \n",
    "    for di, doc in enumerate(l_docs[0:]):    \n",
    "        for sent in list(doc.sents)[0:]:\n",
    "            sentenceObj = {}\n",
    "            sentenceObj['sentence']=sent.text\n",
    "            words = []\n",
    "\n",
    "            for token in sent:        \n",
    "                wordObj = {\n",
    "                    'tokenText':token.text,\n",
    "                     'tokenTag':token.tag_,\n",
    "                     'tokenDep':token.dep_,\n",
    "                     'tokenHeadText':token.head.text,\n",
    "                     'tokenHeadTag':token.head.tag_\n",
    "                }\n",
    "                words.append(wordObj)\n",
    "                #print(\"{0}/{1} <--{2}-- {3}/{4}\".format(token.text, token.tag_, token.dep_, token.head.text, token.head.tag_))\n",
    "\n",
    "            sentenceObj['words'] = words\n",
    "            sentences.append(sentenceObj)\n",
    "\n",
    "        docObject = {}\n",
    "        docObject['id']=di\n",
    "        docObject['sentences']=sentences\n",
    "        dependencyDocuments.append(docObject)\n",
    "    \n",
    "    return dependencyDocuments\n",
    "\n",
    "\n",
    "\n",
    "def create_parse_files(l_docs, writeFile = True, pathToFolders=''):\n",
    "    \"\"\" Creates parse files and stores them in the data/proecssed folder when writeFile=True and pathToFolders is provided\n",
    "        The following file types are created\n",
    "            * dep -- for dependencies\n",
    "            * input -- for POS tagging\n",
    "            * morph -- lemmatized words\n",
    "    \"\"\"\n",
    "    \n",
    "    d_documentData = {\n",
    "        'depData' : [],\n",
    "        'posData' : [],\n",
    "        'morData' : []\n",
    "    }\n",
    "    \n",
    "    for di, doc in enumerate(l_docs[0:]):\n",
    "\n",
    "        l_depSentences = [] # for dependencies\n",
    "        l_posSentences = [] # for POS tagging\n",
    "        l_morSentences = [] # for morphology/lemmatization \n",
    "        \n",
    "        for sent in list(doc.sents)[0:]:\n",
    "            \n",
    "            l_depTokens=[]\n",
    "            l_posTokens=[]\n",
    "            l_morTokens=[]\n",
    "            \n",
    "            for token in sent:\n",
    "                \n",
    "                ## For dependency trees\n",
    "                childTokenPosition = token.i - sent.start  + 1\n",
    "                headTokenPosition =  token.head.i - sent.start +1 \n",
    "\n",
    "                if token.dep_ not in ['ROOT','punct']:\n",
    "                    l_depTokens.append(\"{0}({1}-{2}, {3}-{4})\".format(token.dep_, token.head.text, headTokenPosition, token.text, childTokenPosition ))\n",
    "\n",
    "                ## For POS\n",
    "                l_posTokens.append(\"{0}_{1}\".format(token, token.tag_))  \n",
    "                #print(token.tag_)\n",
    "\n",
    "                ## For Morphologies\n",
    "                l_morTokens.append(token.lemma_)\n",
    "\n",
    "\n",
    "            l_depSentences.append(\"\\n\".join(l_depTokens))\n",
    "            l_posSentences.append(\"\\n\".join(l_posTokens))\n",
    "            l_morSentences.append(\"\\n\".join(l_morTokens))\n",
    "    \n",
    "        d_documentData['depData'].append(l_depSentences)\n",
    "        d_documentData['posData'].append(l_posSentences)\n",
    "        d_documentData['morData'].append(l_morSentences)\n",
    "\n",
    "        if writeFile:\n",
    "            with open(pathToFolders+'\\\\dep\\\\{0:04d}.dep'.format(di), \"w\", encoding='utf8') as text_file:\n",
    "                text_file.write('\\n\\n'.join(l_depSentences))\n",
    "            with open(pathToFolders+'\\\\input\\\\{0:04d}.input'.format(di), \"w\", encoding='utf8') as text_file:\n",
    "                text_file.write('\\n\\n'.join(l_posSentences))\n",
    "            with open(pathToFolders+'\\\\morph\\\\{0:04d}.morph'.format(di), \"w\", encoding='utf8') as text_file:\n",
    "                text_file.write('\\n\\n'.join(l_morSentences))\n",
    "            \n",
    "            print('Files written to folder:', pathToFolders)\n",
    "    return d_documentData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Parse data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of documents:  100\n"
     ]
    }
   ],
   "source": [
    "allDocs = load_data(picklePath='../../data/pickle/NLPifiedDocs-first100.pkl')\n",
    "documentData = create_parse_files(allDocs, True, '..\\\\..\\\\data\\\\processed')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
