{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTIVAC Parsing Text and LaTeX equations\n",
    "\n",
    "<b>PACKAGE DEPENDENCIES (AND INSTALL DIRECTIONS):</b> \n",
    "    \n",
    "* stanfordnlp - https://github.com/stanfordnlp/stanfordnlp\n",
    "* spaCy - https://spacy.io/usage\n",
    "* sympy - https://docs.sympy.org/latest/install.html\n",
    "* antlr-python-runtime - https://anaconda.org/conda-forge/antlr-python-runtime\n",
    "\n",
    "\n",
    "<b>INPUTS:</b> \n",
    "    \n",
    "* JSON file from web scrape\n",
    "\n",
    "\n",
    "<b>OUTPUTS:</b> \n",
    "    \n",
    "* JSON file with tokenized LaTeX equations\n",
    "* dependency (*.dep) files\n",
    "* input (*.input) files\n",
    "* morphology (*.morph) files\n",
    "\n",
    "<b>SUMMARY:</b> \n",
    "    \n",
    "The parsing component of MULTIVAC takes a JSON file of scraped journal articles, and parses the text as well as LaTeX notation contained within the text. LaTeX equation parsing requires the `sympy` library and `antlr` runtime engine. Scripts in `equationparsing.py` use regular expressions to identify the occurrence of properly formatted LaTeX code, and the `sympy` library is used to parse these into string representations.  The tokens from these equations are expanded out in string representation and replace the LaTeX notation in the original text. The text files are written out to `articles-with-equations.json` for further use in preparing GloVe embeddings. \n",
    "\n",
    "The text parsing relies on two natural language processing engines – `stanfordnlp` and `spaCy` – to construct dependency trees, tag parts of speech and lemmatize tokens. Each sentence is processed individually to identify the dependency structure of its tokens. When LaTeX notation occurs in text, its string representation is parsed into dependency trees and included in the sentence structure. Text files are written out to three separate folders: `dep` (contains *.dep* files with Standford dependency trees); `morph` (contains *.morph* files with lemmatized tokens); and `input` (contains *.input* files with parts-of-speech tagged tokens). \n",
    "\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Load packages/libraries:</b> This block loads standard, third-party and local application modules\n",
    "   \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import standard libraries\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Instructions:</b> \n",
    "Add a system path that points to files in `src/data/` for custom modules\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import copy\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import re as reg\n",
    "import sys\n",
    "\n",
    "sys.path.append(os.path.abspath('../../src/data'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import third party libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import stanfordnlp\n",
    "from interruptingcow import timeout"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import local application libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import equationparsing as eq\n",
    "from textparsing import clean_doc\n",
    "from parsing import create_parse_files, get_adjustment_position, get_token_governor, load_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Load natural language processors:</b> \n",
    "    \n",
    "* spaCy is used for quick validation of words (particularly in the `clean_doc` function in the `textparsing` module)\n",
    "* stanfordNLP is used for tokenizing, dependency parsing, POS-tagging and lemmatization\n",
    "    \n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Instructions:</b> \n",
    "Ensure that the path passed to the `models_dir` argument in the `stanfordnlp.Pipeline` method points to the correct location of the StanfordNLP resources on your local machine. \n",
    "    \n",
    "For some installations, spaCy may also require a path.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spacynlp = spacy.load('en_core_web_sm')\n",
    "nlp = stanfordnlp.Pipeline(models_dir='../../../../multivac/stanfordnlp_resources/', \n",
    "                            treebank='en_ewt', use_gpu=False, pos_batch_size=3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Load documents:</b> \n",
    "Retrieve the JSON file that contains article data and metadata\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Instructions:</b> \n",
    "Ensure that the path passed to the `load_data` function matches the location of the JSON file\n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jsonObj, allDocs = load_data('../../../../multivac/data/20181212.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Clean documents:</b> \n",
    "    \n",
    "Retrieve the text elements from the articles and prepare for further processing. The process first checks to see if a pickle of the cleaned data already exists ( `allDocsClean.pkl` ) - if it exists, the pickle is loaded. Otherwise, texts are cleaned and a pickle file is outputted to the working directory as `allDocsClean.pkl`. Cleaned documents are stored as a list in the `allDocsClean` variable. </div>\n",
    "\n",
    "    \n",
    "The cleaning process attempts to:\n",
    "* remove authors\n",
    "* remove citations\n",
    "* remove URLs\n",
    "* remove emails\n",
    "* remove other metadata artefacts from journal scraping embedded within the text\n",
    "* adjust hyphenated words "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "try: \n",
    "    allDocsClean = pickle.load(open('allDocsClean.pkl', \"rb\" ))\n",
    "    print('Loaded pickle!')\n",
    "except FileNotFoundError:\n",
    "    print('No saved pickle. Starting from scratch.')\n",
    "    allDocsClean= []\n",
    "    percentCompletedMultiple = int(len(allDocs)/10)\n",
    "    for i, doc in enumerate(allDocs):\n",
    "        if i%percentCompletedMultiple == 0: \n",
    "            print('{}% completed'.format(round(i/(len(allDocs))*100, 0)))\n",
    "        allDocsClean.append(clean_doc(doc, spacynlp))\n",
    "\n",
    "    with open('allDocsClean.pkl', 'wb') as f:\n",
    "        pickle.dump(allDocsClean, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Extract LaTeX notation:</b> \n",
    "    \n",
    "Find LaTeX notation and store them in the global dictionary variable, `eq.LATEXMAP`. In `eq.LATEXMAP`, the keys are the tags assigned to the instance of the LaTeX notation identified (in the format `LtxqtnXXXXXX`), and the value is the actual LaTeX code.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocs2 = [eq.extract_and_replace_latex(doc) for docNum, doc in enumerate(allDocsClean)]\n",
    "print('Number of LateX Equations parsed: {}'.format(len(eq.LATEXMAP)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### OPTIONAL - Replace LaTeX code in original text with string representation tokens\n",
    "\n",
    "\n",
    "If the equations can be parsed out into a string representation (using the `sympy` library -- see https://docs.sympy.org/latest/tutorial/manipulation.html), then replace the notation with the actual tokens used. The tokens are replaced in the text key of the original JSON file. The updated JSON file is then written out to `articles-with-equations.json`. This file is used for creating GloVe embeddings in a later step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "allDocs3 = []\n",
    "percentCompletedMultiple = int(len(allDocs2)/10)\n",
    "for i, doc in enumerate(allDocs2[0:]):\n",
    "    if i%percentCompletedMultiple == 0: \n",
    "        print('{}% completed'.format(round(i/(len(allDocs2))*100, 0)))\n",
    "    newDoc = reg.sub(r'Ltxqtn[a-z]{8}', eq.put_equation_tokens_in_text, doc)\n",
    "    allDocs3.append(newDoc)\n",
    "\n",
    "jsonObj2 = copy.deepcopy(jsonObj)\n",
    "allDocs3Counter = 0 \n",
    "\n",
    "for key, value in list(jsonObj2.items()):\n",
    "    if value['text']:\n",
    "        jsonObj2[key]['text']=allDocs3[allDocs3Counter]\n",
    "        allDocs3Counter = allDocs3Counter+1\n",
    "\n",
    "with open('articles-with-equations.json', 'w', encoding='utf8') as fp:\n",
    "    json.dump(jsonObj2, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "<b>Create dependendy parse, POS-tagged tokens and lemmatized token files:</b> \n",
    "    \n",
    "This process creates the `.dep`, `.input`, and `.morph` files for further processing in the Markov Logic Networks. It processes each text using the Stanford NLP pipeline and saves the output to a specified folder. The NLP-ification process is time consuming and can crash for certain texts that can't be parsed out. A runtime error is raised if the process takes more than 5 minutes and the file is skipped. \n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "<b>Instructions:</b> \n",
    "    \n",
    "Ensure that the path passed to `create_parse_files` matches the desired output directory. The output directory must contain three subfolders:\n",
    "* `dep`\n",
    "* `input`\n",
    "* `morph`\n",
    "\n",
    "Output files will be written to each of these folders. \n",
    "\n",
    "The start point can be adjusted using the `startPoint` variable. When set to 0, it starts with the first text. This can be useful for pausing and coming back to the process (e.g., between system reboots). \n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "startPoint=950\n",
    "\n",
    "for i, doc in enumerate(allDocs2[0:]):\n",
    "    print('Processing document #{}'.format(i))\n",
    "    if i > startPoint:\n",
    "\n",
    "        # Use exception handling so that the process doesn't get stuck and time out because of memory errors\n",
    "        try:\n",
    "            with timeout(300, exception=RuntimeError):\n",
    "                nlpifiedDoc = nlp(doc)\n",
    "                thisDocumentData = create_parse_files(nlpifiedDoc, i, True, 'output_data/')\n",
    "        except RuntimeError:\n",
    "            print(\"Didn't finish document #{} within five minutes. Moving to next one.\".format(i))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
