{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load in Cleaned Article Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in in the parsed article files in .json format and output as a dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from unidecode import unidecode\n",
    "\n",
    "with open(\"/Users/taylor_bolt/Desktop/ASKE_MULTIVAC/cleanedArticles-03042019.json\", 'r', encoding='utf-8') as jf:\n",
    "    src_data = json.load(jf)\n",
    " \n",
    "texts = [src_data[art]['text'] for art in src_data if src_data[art]['text'] is not None]\n",
    " \n",
    "# The \"unidecode\" step simplifies non-ASCII chars which\n",
    "# mess up the R GloVe engine. Probably a more sophisticated way to\n",
    "# bridge that gap but this is the quick and dirty solution\n",
    " \n",
    "texts_df = pd.Series(texts).apply(lambda x: unidecode(x))\n",
    "texts_df = pd.DataFrame({'text':texts_df})\n",
    "del texts, src_data\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Create Domain-Specific GloVe Embedding Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create the domain-specific (DS) GloVe embedding model from the parsed journal article text. This is run from the 'trainEmbeddings.R' script, but called from Python using rpy2. This takes a while, so for quickness, we load in a previously run script. Given the size of the vocabulary, this process takes a lot of memory usage. We recommend that this process is run on a computer with at least 16GB of RAM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from rpy2.robjects import r, pandas2ri, numpy2ri\n",
    "\n",
    "# # Source all the functions contained in the 'trainEmbeddings' R file\n",
    "# r(\"source('trainEmbeddings.R')\")\n",
    "# # Call the main GloveEmbedding function from the R script\n",
    "# trainEmbeddings_R = r(\"trainEmbeddings\")\n",
    "# # Train DS GloVe Embedding model and ouput as a Numpy Matrix\n",
    "# pandas2ri.activate()\n",
    "# DS_embeddings_R = trainEmbeddings_R(texts_df)\n",
    "# del texts_df\n",
    "# pandas2ri.deactivate()\n",
    "# DS_embeddings = numpy2ri.rpy2py(DS_embeddings_R[0])\n",
    "# # Get DS GloVe vocabulary\n",
    "# domain_spec_vocab = list(DS_embeddings_R[1])\n",
    "# del DS_embeddings_R\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Import Domain-General GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Load in the domain-general (DG) GloVe embedding model from the 'Common Crawl' pre-trained model from Stanford (https://nlp.stanford.edu/projects/glove/). This is saved as a .txt on disk. This takes a while to load (~30min). Thus, repeatedly loading this in is fairly inefficient. Rather, I have previously run this script and saved the relevant parts of the model to a pickle file on disk. However, I provide the code commented out below to load in the data if necessary (most relevant is the loadGloveModel function)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "\n",
    "## Load in Stanford's 'Common Crawl' Domain-General Glove Embedding Model\n",
    "# Only pull out the words that are contained in our corpus\n",
    "# * This can take a while (~30min) - could use some optimization * \n",
    "\n",
    "# def loadGloveModel(gloveFile):\n",
    "#     f = open(gloveFile,'r')\n",
    "#     model = {}\n",
    "#     for line in f:\n",
    "#         splitLine = line.split()\n",
    "#         word = splitLine[0]\n",
    "#         if word in domain_spec_vocab:\n",
    "#             #print(word)\n",
    "#             embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "#             model[word] = embedding\n",
    "#     return model\n",
    "# DG_embeddings = loadGloveModel('/Users/taylor_bolt/Desktop/ASKE_MULTIVAC/glove.42B.300d.txt')\n",
    "\n",
    "# * For now, we just load in a previously saved 'model subset' *\n",
    "DG_embeddings = pickle.load( open( \"/Users/taylor_bolt/Desktop/ASKE_MULTIVAC/model_subset_DG.p\", \"rb\" ) )\n",
    "domain_gen_vocab = list(DG_embeddings.keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run CCA b/w Domain-General and Domain-Specific GloVe Embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> Create the domain-adapted (DA) GloVe embeddings the CCA between on the tokens that are shared in common between the DG and DS vocabulary. The vectors for each token of the DA glove embedding model are derived from a weighted average of the canonical vectors (N = 100) from the CCA analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48534, 300)\n",
      "(48534, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor_bolt/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:79: ConvergenceWarning: Maximum number of iterations reached\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import zscore \n",
    "from sklearn.cross_decomposition import CCA\n",
    "\n",
    "## Processing to ensure rows match between the DG and DS embeddings\n",
    "# Convert domain general (DG) embedding from dictionary to array\n",
    "DG_embeddings = np.array([DG_embeddings[i] for i in DG_embeddings.keys()])\n",
    "# Find the indices of matching words\n",
    "both = set(domain_gen_vocab).intersection(domain_spec_vocab)\n",
    "indices_gen = [domain_gen_vocab.index(x) for x in both]\n",
    "indices_spec = [domain_spec_vocab.index(x) for x in both]\n",
    "indices_spec_notDG = [domain_spec_vocab.index(x) for x in domain_spec_vocab if x not in both]\n",
    "\n",
    "# Sort and subset domain specific (DS) array to match indices of DG array\n",
    "DS_embeddings_subset = DS_embeddings[indices_spec,:].copy()\n",
    "DG_embeddings_subset = DG_embeddings[indices_gen,:].copy()\n",
    "\n",
    "\n",
    "def domain_adapted_CCA(DG_embed,DS_embed,NC=100):\n",
    "    # Z-score\n",
    "    DG_embed_norm = zscore(DG_embed)\n",
    "    print(DG_embed_norm.shape)\n",
    "    DS_embed_norm = zscore(DS_embed)\n",
    "    print(DS_embed_norm.shape)\n",
    "    # Initialize CCA Model\n",
    "    cca = CCA(n_components=NC)\n",
    "    cca.fit(DG_embed_norm,DS_embed_norm)\n",
    "    \n",
    "    DA_embeddings = (cca.x_scores_ + cca.y_scores_)/2\n",
    "    return cca, DA_embeddings\n",
    "\n",
    "cca_res, DA_embeddings = domain_adapted_CCA(DG_embeddings_subset,DS_embeddings_subset,NC=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Project Left-Out Domain-Specific Tokens on to Domain-Adapted Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The tokens of the DS embedding model that are left out of the intersection between the DS and DG embedding model are projected into the 100-dimensional canonical vector space from the CCA analysis (via matrix multiplication) and appended to the DA embedding vectors (created above). Your final output is 'DA_embeddings_final'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "DS_embeddings_notinDG = DS_embeddings[indices_spec_notDG,:]\n",
    "DS_embeddings_notinDG_norm = zscore(DS_embeddings_notinDG)\n",
    "\n",
    "DA_notinDG_embeddings = cca_res.y_weights_.T @ DS_embeddings_notinDG_norm.T\n",
    "DA_embeddings_final = np.append(DA_embeddings,DA_notinDG_embeddings.T,axis=0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
