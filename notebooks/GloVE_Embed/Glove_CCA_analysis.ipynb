{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Parse Arxiv PDFs\n",
    "\n",
    "Below parses the pdfs into text, while attempting to remove the title and reference pages. For some pdfs, the title and reference pages cannot be removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_all_pdfs(pdf_dir):\n",
    "    import slate\n",
    "    import sys\n",
    "    import glob, os\n",
    "    import logging\n",
    "    logging.propagate = False \n",
    "    logging.getLogger().setLevel(logging.ERROR)\n",
    "    \n",
    "    os.chdir(pdf_dir)\n",
    "    text_all=[]\n",
    "    files = glob.glob(\"*.pdf\")\n",
    "    for file in glob.glob(\"*.pdf\"):\n",
    "        print(file)\n",
    "        try:\n",
    "            with open(pdf_dir + '/' + file,'rb') as f:\n",
    "                doc = slate.PDF(f)\n",
    "        except:\n",
    "            print('reading of ' + file + ' failed')\n",
    "            continue\n",
    "        doc = ' '.join([' '.join(x.split()) for x in doc])\n",
    "        ## Try to get rid of irrelevant parts of the text (Title Page, References, Appendix)\n",
    "        # Remove title page\n",
    "        text_split = doc.lower().split(' abstract ')\n",
    "        if len(text_split)>1: \n",
    "            text_no_title = ' '.join(text_split[1:])\n",
    "        else: # if no abstract text, try to get introduction onward\n",
    "            text_split = doc.lower().split(' introduction ')\n",
    "    \n",
    "            if len(text_split)>1:\n",
    "                text_no_title = ' '.join(text_split[1:])\n",
    "            else: # if still can't get any split, just give up and take the title page onward\n",
    "                text_no_title = text_split[0]\n",
    "                print(\"can't remove title page :(\")\n",
    "                \n",
    "        # Remove reference page\n",
    "        text_split = text_no_title.lower().split(' acknowledgements ')\n",
    "        if len(text_split)>1:\n",
    "            text_no_title_ref = ' '.join(text_split[:-1])\n",
    "        else: # sometimes 'acknowledgements' is spelled 'acknowledgments' without the 'e'\n",
    "            text_split = text_no_title.lower().split(' acknowledgments ')\n",
    "            \n",
    "            if len(text_split)>1: \n",
    "                text_no_title_ref = ' '.join(text_split[:-1])\n",
    "            else: # If no acknowledgements section try to get references\n",
    "                text_split = text_no_title.lower().split(' references ')\n",
    "                \n",
    "                if len(text_split)>1:\n",
    "                    text_no_title_ref = ' '.join(text_split[:-1])\n",
    "                else: # if still can't get any split, just give up and take reference pages\n",
    "                    text_no_title_ref = text_split[0]\n",
    "                    print(\"can't remove reference pages :(\")\n",
    "        \n",
    "        text_all.append(text_no_title_ref)\n",
    "        \n",
    "    return text_all\n",
    "\n",
    "            \n",
    "# Parse all arxiv pdf articles into text\n",
    "text_all = parse_all_pdfs('/Users/taylor_bolt/Desktop/ASKE_MULTIVAC/arxiv')   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Tokenizing and Pre-processing of PDFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below use SpaCy to preprocess each pdf text. Note, we exclude 'uninformative' parts of speech - punctuation, particles, numbers, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor_bolt/anaconda3/lib/python3.7/site-packages/requests/__init__.py:91: RequestsDependencyWarning: urllib3 (1.24.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "def spacy_preprocess(text):\n",
    "    import spacy\n",
    "    nlp = spacy.load('en_core_web_sm',disable=['parser','ner'])\n",
    "    doc_final = []\n",
    "    select_pos = ['ADJ','NOUN','ADV','VERB','ADP','PROPN']\n",
    "\n",
    "    # Loop through all reviews, lemmatize words and select parts of speech with SpaCy\n",
    "    for i in text:\n",
    "        doc_new = nlp(i)\n",
    "        doc_new = [token.lower_ for token in doc_new if token.pos_ in select_pos if len(token)<20 if token.is_alpha]\n",
    "        doc_final.append(doc_new)\n",
    "    return doc_final\n",
    "\n",
    "# Load all text saved in pickle object and apply spacy preprocessing\n",
    "import pickle\n",
    "pickle_in = open(\"/Users/taylor_bolt/Desktop/ASKE_MULTIVAC/GloVe_CCA/arxiv_text_all.pickle\",\"rb\")\n",
    "text_all = pickle.load(pickle_in)\n",
    "text_prep = spacy_preprocess(text_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Create Word Co-Occurence Matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input to the GloVe Embedding Model requires a co-occurence matrix. We essentially slide a fixed window size (n = 10 words) across each parse pdf, and count the co-occurences between all the words of our vocabulary.\n",
    "\n",
    "Note, I had to limit the vocabulary to 20,000 words to get the GloVe embedding model to converge in a reasonable time (the entire vocab is about 40,000 words)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def co_occur_matrix(text,window_size=10,overlap=9,N_sub=20000):\n",
    "    from sklearn.feature_extraction.text import CountVectorizer\n",
    "    import numpy as np\n",
    "    import random\n",
    "    # Create dummy function to 'fool' CountVectorizer into not tokenizing an already tokenized list\n",
    "    def dummy_fun(text):\n",
    "          return text\n",
    "    random.seed(1) # make sure we get the same random numbers again for debugging\n",
    "    # Find vocabulary of words from all text\n",
    "    vocab = list(set([y for x in text for y in x]))\n",
    "    # Have to downsample vocabulary for GloVe estimation\n",
    "    rand_indx = random.sample(list(range(len(vocab))),N_sub)\n",
    "    vocab_subset = [vocab[i] for i in rand_indx]\n",
    "    # Initialize count model\n",
    "    count_model = CountVectorizer(analyzer=\"word\",tokenizer=dummy_fun,preprocessor=dummy_fun,\n",
    "                                  vocabulary=vocab_subset,token_pattern=None) \n",
    "\n",
    "    # Loop through all documents provided\n",
    "    for loop_indx,doc in enumerate(text):\n",
    "        #print(loop_indx)\n",
    "        # If this is the first document, initialize the word-co-occurence \n",
    "        if loop_indx==0:\n",
    "            # Get all windows of specifized size from document\n",
    "            windows_all = [doc[i:i+overlap] for i in range(0, len(doc), window_size-overlap)]\n",
    "            X = count_model.fit_transform(windows_all)\n",
    "            # Set all occurences to 1, just in case the same word appears more than once in a window\n",
    "            X[X>0] = 1\n",
    "            # Compute Co-occurence matrix w/ matrix multiplication\n",
    "            Xc = (X.T * X)\n",
    "        # After the first document, add each document co-occurence to overall co-occurence matrix\n",
    "        else: \n",
    "            # Get all windows of specifized size from document\n",
    "            windows_all = [doc[i:i+overlap] for i in range(0, len(doc), window_size-overlap)]\n",
    "            X = count_model.fit_transform(windows_all)\n",
    "            # Set all occurences to 1, just in case the same word appears more than once in a window\n",
    "            X[X>0] = 1\n",
    "            # Compute Co-occurence matrix w/ matrix multiplication\n",
    "            Xc_temp = (X.T * X)\n",
    "            Xc = Xc + Xc_temp\n",
    "    \n",
    "    return Xc, vocab_subset\n",
    "            \n",
    "Xc, vocab = co_occur_matrix(text_prep)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Run GloVe Model on Pre-processed Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We estimate 300 dimensions in the GloVe model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mittens import GloVe\n",
    "Xc_array = Xc.toarray()\n",
    "glove_model = GloVe(n=300, max_iter=200)  # 300 is the embedding dimension\n",
    "embeddings = glove_model.fit(Xc_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Load in Pretrained GloVe Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To combine the pretrained Glove Model and our domain specific Glove Model we need to find the intersection between the two vocabularies. We load in that vocabulary of the pretrained GloVe model that is also in our domain specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if word in vocab:\n",
    "            print(word)\n",
    "            embedding = np.array([float(val) for val in splitLine[1:]])\n",
    "            model[word] = embedding\n",
    "    return model\n",
    "model_subset = loadGloveModel('glove.42B.300d.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. CCA between Pretrained and Domain-specific GloVe Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(14630, 300)\n",
      "(14630, 300)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/taylor_bolt/anaconda3/lib/python3.7/site-packages/sklearn/cross_decomposition/pls_.py:79: ConvergenceWarning: Maximum number of iterations reached\n",
      "  ConvergenceWarning)\n"
     ]
    }
   ],
   "source": [
    "def domain_adapted_CCA(DG_embed,DS_embed,DS_vocab,NC=100):\n",
    "    from scipy.stats import zscore\n",
    "    import numpy as np\n",
    "    from sklearn.cross_decomposition import CCA\n",
    "    # Convert domain general (DG) embedding from dictionary to array\n",
    "    DG_embed_new = np.array([DG_embed[i] for i in DG_embed.keys()])\n",
    "    # Find the indices of matching words\n",
    "    indx_match = []\n",
    "    for i, x in enumerate(DG_embed.keys()):\n",
    "        for j, y in enumerate(DS_vocab):\n",
    "            if x == y:\n",
    "                indx_match.append([i, j])\n",
    "    # Sort domain specific (DS) embedding array to match indices of DG array\n",
    "    sort_indx = np.array([i[1] for i in indx_match]).argsort()\n",
    "    DS_embed_new = DS_embed[sort_indx,:]\n",
    "    # Transpose both and z-score\n",
    "    DG_embed_norm = zscore(DG_embed_new)\n",
    "    print(DG_embed_norm.shape)\n",
    "    DS_embed_norm = zscore(DS_embed_new)\n",
    "    print(DS_embed_norm.shape)\n",
    "    # Initialize CCA Model\n",
    "    cca = CCA(n_components=NC)\n",
    "    cca.fit(DG_embed_norm,DS_embed_norm)\n",
    "    \n",
    "    return cca\n",
    "\n",
    "# # Load in previously estimated GloVe model on Arxiv Articles\n",
    "# import pickle \n",
    "# pickle_in = open(\"DA_embedding_D300.dat\",\"rb\")\n",
    "# pickle_output = pickle.load(pickle_in)\n",
    "# DS_embed = pickle_output[0]\n",
    "# DS_vocab = pickle_output[1]\n",
    "\n",
    "# # Load in Pre-trained GloVe that was processed earlier\n",
    "# import pickle \n",
    "# pickle_in = open(\"glove_pretrained_subset.pickle\",\"rb\")\n",
    "# pickle_output = pickle.load(pickle_in)\n",
    "# DG_embed = pickle_output\n",
    "\n",
    "#cca_res = domain_adapted_CCA(DG_embed,DS_embed,DS_vocab,NC=10)\n",
    "\n",
    "cca_res = domain_adapted_CCA(model_subset,embeddings,vocab,NC=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.        , 0.27892825],\n",
       "       [0.27892825, 1.        ]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.corrcoef(cca_res.y_scores_[:,0],cca_res.x_scores_[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
