{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext dotenv\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "# move current working directory up two levels to root\n",
    "# not pretty but this is a notebook\n",
    "# don't run this cell more than once or you'll move another two directories up, which wouldn't be good\n",
    "os.chdir(os.pardir); os.chdir(os.pardir); os.chdir(os.pardir)\n",
    "print('Current working directory is %s' % os.getcwd())\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "from multivac.src import utilities\n",
    "from multivac import settings\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from collections import OrderedDict\n",
    "import copy\n",
    "import re\n",
    "import feedparser\n",
    "import pubmed_parser\n",
    "import time\n",
    "import slate\n",
    "import re\n",
    "import pickle\n",
    "import json\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_pdf(src):\n",
    "    \"\"\"Parse \"\"\"\n",
    "    try:\n",
    "        # try to open file\n",
    "        with open(src, 'rb') as f:\n",
    "            doc = slate.PDF(f)\n",
    "\n",
    "        # get text: strip out newlines and extra spaces\n",
    "        doc = ' '.join([' '.join(x.split()) for x in doc])\n",
    "        text = doc.split(' Abstract ')[-1].split(' Acknowledgments ')[0].split(' ∗ ∗ ∗ ')[0].strip()\n",
    "\n",
    "    except:  #  PDFSyntaxError\n",
    "        text = None\n",
    "\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_html(src):\n",
    "\n",
    "    with open(src, 'r', encoding='utf-8') as f:\n",
    "        raw_data_ = f.read()\n",
    "    soup = bs(raw_data_)\n",
    "    try:\n",
    "        text = ' '.join(soup.find('article').get_text().split())\n",
    "    except AttributeError:\n",
    "        text = None\n",
    "    return text\n",
    "\n",
    "\n",
    "def parse_articles_data(source, data_raw_dir, verbose=False):\n",
    "    \"\"\"Parse Arxiv and Springer article data.\"\"\"\n",
    "    # load metadata\n",
    "    fn = source + '.pkl'\n",
    "    metadata_src = settings.metadata_dir / fn\n",
    "    with open(metadata_src, 'rb') as f:\n",
    "        metadata_ = pickle.load(f)\n",
    "\n",
    "    # we'll just add the text to a new arxiv object, an ordered dict keyed on doi or other id\n",
    "    data = OrderedDict()\n",
    "    srcs = [data_raw_dir / x for x in os.listdir(data_raw_dir)]\n",
    "    for ix, article_metadata in enumerate(metadata_):\n",
    "        \n",
    "        # initialize temp dictionary\n",
    "        temp = OrderedDict()\n",
    "        temp['metadata'] = copy.deepcopy(article_metadata)\n",
    "        temp['metadata']['source'] = source\n",
    "        article_fn = article_metadata['fn']\n",
    "        if verbose:\n",
    "            print(article_fn)\n",
    "        src = data_raw_dir / article_fn\n",
    "    \n",
    "        # define key and value\n",
    "        if source == 'arxiv':\n",
    "            k = article_metadata['fn'].strip('.pdf')\n",
    "            temp['text'] = parse_pdf(src)\n",
    "        elif source =='springer':\n",
    "            k = article_metadata['doi']\n",
    "            temp['text'] = parse_html(src)\n",
    "        elif source == 'pubmed':\n",
    "            raise ValueError('pubmed not supported. Only \"arxiv\" and \"springer\" supported. Try \"parse_pubmed() function\"')\n",
    "        else:\n",
    "            raise ValueError('Only \"arxiv\" and \"springer\" supported as sources.')\n",
    "        \n",
    "        # populate interim dictionary\n",
    "#         if len()\n",
    "        data[k] = temp\n",
    "    \n",
    "    # save intermediate outputs\n",
    "    data_interim_dst = settings.interim_dir / fn\n",
    "    with open(data_interim_dst, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    return data\n",
    "\n",
    "\n",
    "def parse_pubmed(src):\n",
    "    \"\"\"Parse pubmed xml article data and return metadata and text.\"\"\"\n",
    "    metadata = pubmed_parser.parse_pubmed_xml(src)\n",
    "    text = pubmed_parser.parse_pubmed_paragraph(src, all_paragraph=True)\n",
    "    text = ' '.join(' '.join([x['text'] for x in text]).split())\n",
    "    return metadata, text\n",
    "\n",
    "\n",
    "def aggregate_pubmed(srcs, verbose=False):\n",
    "    \"\"\"Aggregate a set of Pubmed article text and metadata.\"\"\"\n",
    "    pubmed_data = OrderedDict()\n",
    "    pubmed_metadata = OrderedDict()\n",
    "    for src in srcs:\n",
    "        if verbose:\n",
    "            print(src)\n",
    "        try:\n",
    "            temp = OrderedDict()\n",
    "            metadata, text = parse_pubmed(str(src.absolute()))\n",
    "            temp['metadata'] = metadata\n",
    "            temp['metadata']['source'] = 'pubmed'\n",
    "            temp['text'] = text\n",
    "            try:\n",
    "                k = metadata['doi']\n",
    "            except AttributeError:\n",
    "                k = src.strip('.xml')\n",
    "            if len(text) > 0:\n",
    "                pubmed_data[k] = temp\n",
    "                pubmed_metadata[k] = metadata\n",
    "            print(src)\n",
    "        except:\n",
    "            if verbose:\n",
    "                print('Error: %s' % src)\n",
    "            pass\n",
    "    dst = settings.metadata_dir / 'pubmed.pkl'\n",
    "    with open(dst, 'w') as f:\n",
    "        pickle.dump(pubmed_metadata, f)\n",
    "    return pubmed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = {}\n",
    "for source in settings.sources:\n",
    "    print(source)\n",
    "    fn = source + '.pkl'\n",
    "    data_raw_dir = settings.raw_dir / source\n",
    "    if source in ['arxiv', 'springer']:\n",
    "        data = parse_articles_data(source, data_raw_dir)\n",
    "    elif source == 'pubmed':\n",
    "        srcs = [data_raw_dir / x for x in os.listdir(data_raw_dir)]\n",
    "        data = aggregate_pubmed(srcs)\n",
    "        \n",
    "    if len(output) == 0:\n",
    "        output = copy.deepcopy(data)\n",
    "    else:\n",
    "        output.update(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "arxiv_drops = [x.split()[0] for x in settings.arxiv_drops]\n",
    "def filter_arxiv(output, arxiv_drops):\n",
    "    filtered_output = OrderedDict()\n",
    "    for k, v in output.items():\n",
    "        if v['metadata']['source'] == 'arxiv':\n",
    "            for term in v['metadata']['tags']:\n",
    "                if term['term'] not in arxiv_drops:\n",
    "                    filtered_output[copy.deepcopy(k)] = copy.deepcopy(v)\n",
    "        else:\n",
    "            filtered_output[copy.deepcopy(k)] = copy.deepcopy(v)\n",
    "    return filtered_output\n",
    "filtered_output = filter_arxiv(output, arxiv_drops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_outputs(output, dst_dir=None, fn_prefix=None):\n",
    "    if dst_dir is None:\n",
    "        dst_dir = settings.processed_dir / 'data'\n",
    "    utilities.mkdir(dst_dir)\n",
    "    fn = 'data.json'\n",
    "    if fn_prefix is not None:\n",
    "        fn = fn_prefix + '_' + fn\n",
    "    dst = dst_dir / fn\n",
    "    with open(dst, 'w') as f:\n",
    "        json.dump(output, f)\n",
    "\n",
    "save_outputs(filtered_output)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "multivac",
   "language": "python",
   "name": "multivac"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
